[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yekta Amirkhalili - Dissertation Website",
    "section": "",
    "text": "Hi! Welcome to my dissertation website. I started my PhD in Management Science & Engineering at University of Waterloo in August 2021. This website will walk you through my entire project, which took me over 3 years to finish. I‚Äôm hoping this project can be used by others as inspiration, and hopefully, as a portfolio project for my own! I find a lot of this stuff fun! I‚Äôm also hoping to make everything super accessible to everyone, even if you‚Äôre coming from no background in Statistics/Math/Computer Science.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My Name is Yekta (read exactly how you write it), my pronouns are She/Her. I was born in Tehran, Iran. Since I was a kid, I‚Äôve always loved learning and adventures. Movies are my first passion! After that, it‚Äôs TV shows, music, gadgets and technology, books, writing and studying! Yes! I actually like studying!\nI quickly found out I was much better at Math and Physics than Science and any other field that required memorizing facts (History). I wanted to be an astronaut, then an actress, then a filmmaker, then a writer, then a super model (plans didn‚Äôt work out, I stopped growing taller at 14!), then a robotics engineer, then a computer engineer, then an electrical engineer, then a photographer, etc. I thought actor was the one that made the most sense, since I wanted to be so many things! At least with acting, I could pretend to be all of those things!\nWell, I chose mathematics in highschool, and since I was good at it, I continued with mathematics to college. Big Mistake (For Me)! Studying math in undergrad at University of Tehran was like studying Chinese, Greek, Russian and Arabic at the same time. You don‚Äôt see numbers after Calculus 2, which you take in the second semester/term. All abstract ideas, proofs, theorems, n-dimensional spaces and curves that curve inside of themselves! ‚ÄúI should have studied Accounting!‚Äù (Which I did, btw! I actually took an entire accounting training course outside of university and found it to be sould-crushingly boring, no offense to all accountants.)\nOk, so if math isn‚Äôt about numbers and I don‚Äôt actually want to work with numbers - then where do I go?! So, thankfully, at UT, I was able to take most of my electives courses from the two other majors in the math department: computer science and statistics. This was the turning point for me. ‚ÄúHere‚Äôs where that theory applies, you know, the one you kept asking why you‚Äôre even learning its proof‚Äù‚Ä¶ So I figured I should change my major. Mathematics had taught me nothing but how to think, how to analyze, how to be logical and how important it was to know why stuff made sense‚Ä¶ so really, nothing much! I didn‚Äôt even realize how much studying mathematics had taught me until years later.\nI still changed my major for graduate school, and decided to try Industrial & Systems Engineering, with a specialization on Macrosystems engineering. I taught myself Python and SQL, I did very well during graduate school with my courses, I worked on research projects and had the best time writing my thesis. So I thought, wow, I actually like all of this! Why not do a PhD? And here I am! I love learning, getting deep in a topic, planning a project from the start to the end."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "Hi! My Name is Yekta (read exactly how you write it), my pronouns are She/Her. I was born in Tehran, Iran. Since I was a kid, I‚Äôve always loved learning and adventures. Movies are my first passion! After that, it‚Äôs TV shows, music, gadgets and technology, books, writing and studying! Yes! I actually like studying!\nI quickly found out I was much better at Math and Physics than Science and any other field that required memorizing facts (History). I wanted to be an astronaut, then an actress, then a filmmaker, then a writer, then a super model (plans didn‚Äôt work out, I stopped growing taller at 14!), then a robotics engineer, then a computer engineer, then an electrical engineer, then a photographer, etc. I thought actor was the one that made the most sense, since I wanted to be so many things! At least with acting, I could pretend to be all of those things!\nWell, I chose mathematics in highschool, and since I was good at it, I continued with mathematics to college. Big Mistake (For Me)! Studying math in undergrad at University of Tehran was like studying Chinese, Greek, Russian and Arabic at the same time. You don‚Äôt see numbers after Calculus 2, which you take in the second semester/term. All abstract ideas, proofs, theorems, n-dimensional spaces and curves that curve inside of themselves! ‚ÄúI should have studied Accounting!‚Äù (Which I did, btw! I actually took an entire accounting training course outside of university and found it to be sould-crushingly boring, no offense to all accountants.)\nOk, so if math isn‚Äôt about numbers and I don‚Äôt actually want to work with numbers - then where do I go?! So, thankfully, at UT, I was able to take most of my electives courses from the two other majors in the math department: computer science and statistics. This was the turning point for me. ‚ÄúHere‚Äôs where that theory applies, you know, the one you kept asking why you‚Äôre even learning its proof‚Äù‚Ä¶ So I figured I should change my major. Mathematics had taught me nothing but how to think, how to analyze, how to be logical and how important it was to know why stuff made sense‚Ä¶ so really, nothing much! I didn‚Äôt even realize how much studying mathematics had taught me until years later.\nI still changed my major for graduate school, and decided to try Industrial & Systems Engineering, with a specialization on Macrosystems engineering. I taught myself Python and SQL, I did very well during graduate school with my courses, I worked on research projects and had the best time writing my thesis. So I thought, wow, I actually like all of this! Why not do a PhD? And here I am! I love learning, getting deep in a topic, planning a project from the start to the end."
  },
  {
    "objectID": "study1.html#part-2.",
    "href": "study1.html#part-2.",
    "title": "The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "section": "Part 2.",
    "text": "Part 2.",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR"
    ]
  },
  {
    "objectID": "study1.html#part-3.",
    "href": "study1.html#part-3.",
    "title": "The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "section": "Part 3.",
    "text": "Part 3.",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR"
    ]
  },
  {
    "objectID": "study1.html#part-2.-data-analysis",
    "href": "study1.html#part-2.-data-analysis",
    "title": "The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "section": "Part 2. Data Analysis",
    "text": "Part 2. Data Analysis\n\nPart 2.1 Data Cleaning and Prep\nThe R libraries used for data analysis are as follows:\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(psych)\nlibrary(tidyr)\nlibrary(stargazer)\nlibrary(forcats)\nlibrary(xtable)\nlibrary(ggraph)\nlibrary(igraph)\nlibrary(gt)\nlibrary(ggpubr)\n\nLooking at the data:\n\ndf &lt;- read.csv(\"data/P2_AR_07.csv\") \nglimpse(df)\n\nSummary statiscs\n\npsych::describe(df %&gt;% \n    dplyr::select(Year, Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\nWoah! One paper has 25,000 and that is messing up the sample sizes. Remembering this study‚Äôs ID:\n\ndf %&gt;% filter(SampleSize == 25000) %&gt;% \n    dplyr::select(ID, Title, SampleSize)\n\nSetting aside the study with sample size of 25,000:\n\npsych::describe(\n    df %&gt;% dplyr::select(Year, Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize) %&gt;% \n    filter(SampleSize != 25000)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\nWelp! Another large study.\n\ndf %&gt;% filter(SampleSize == 21526) %&gt;% \n    dplyr::select(ID, Title, SampleSize)\n\nSetting aside the study with sample size of 25,000 and the one with 21,52 as they are outliers:\n\npsych::describe(\n    df %&gt;% filter(!ID %in% c('p2_59','p2_77')) %&gt;% \n    dplyr::select(ID, Year,Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\nCounting the unique values for each of the columns:\n\nresults &lt;- c(\n  paste('Number of Unique Values in ID: ', n_distinct(df$ID)),\n  paste('Number of Unique Values in Title: ', n_distinct(df$Title)),\n  paste('Number of Unique Values in PublicationTitles: ', n_distinct(df$PublicationTitle)),\n  paste('Number of Unique Values in Publisher: ', n_distinct(df$Publisher)),\n  paste('Number of Unique Values in AffiliationCountry: ', n_distinct(df$AffiliationCountry)),\n  paste('Number of Unique Values in Factors: ', dplyr::n_distinct(df %&gt;% dplyr::select(F1:F9) %&gt;% unlist())),\n  paste('Number of Unique Values in Not Sig: ', dplyr::n_distinct(df %&gt;% dplyr::select(FNS1:FNS4) %&gt;% unlist())),\n  paste('Number of Unique Values in Methods: ', dplyr::n_distinct(df %&gt;% dplyr::select(METHOD1:METHOD4) %&gt;% unlist())),\n  paste('Number of Unique Values in Theory: ', dplyr::n_distinct(df %&gt;% dplyr::select(THEORY1:THEORY4) %&gt;% unlist())),\n  paste('Number of Unique Values in Limits: ', dplyr::n_distinct(df %&gt;% dplyr::select(LIMIT1:LIMIT3) %&gt;% unlist())),\n  paste('Number of Unique Values in ResearchType: ', n_distinct(df$ResearchType)),\n  paste('Number of Unique Values in Authors: ', n_distinct(df$Creators)),\n  paste('Number of Unique Values in Keywords: ', dplyr::n_distinct(df %&gt;% dplyr::select(K1:K10) %&gt;% unlist())),\n  paste('Number of Unique Values in Tech: ', n_distinct(df$Tech)),\n  paste('Number of Unique Values in Themes: ', n_distinct(df$DecisionTheme))\n)\n\ncat(results, sep = \"\\n\")\n\nChecking the sample sizes Without the outliers:\n\npsych::describe(\n    df %&gt;% filter(!ID %in% c('p2_59','p2_77')) %&gt;% \n    dplyr::select(SampleSize)) %&gt;% \n    dplyr::select(n, mean, sd, median, min, max) \n\n\nnoOutliers &lt;- df %&gt;% filter(!ID %in% c('p2_59','p2_77'))\n\nquantiles &lt;- quantile(noOutliers$SampleSize, na.rm = T)\n\nquantile_binned &lt;- cut(df$SampleSize, \n                breaks = quantiles, \n                labels = c(\"SQ1\", \"SQ2\", \"SQ3\", \"SQ4\"), \n                include.lowest = TRUE)\n\ndf$SampleSizeBin &lt;- quantile_binned\n\ndf &lt;- df %&gt;% mutate(\n    SampleSizeBin = if_else(\n        is.na(SampleSizeBin),\n        \"NotStated\",\n        SampleSizeBin\n    )\n)\n\ndf %&gt;% count(SampleSizeBin)\n\nLet‚Äôs calculate the scores for factors that are significant and non-significant:\n\nF_counts &lt;- df %&gt;%\n  dplyr::select(F1:F9) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", F_count = \"Freq\")\n\nFNS_counts &lt;- df %&gt;%\n  dplyr::select(FNS1:FNS4) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", FNS_counts = \"Freq\")\n\n# Count occurrences of each factor in all columns (F1 to F9 + FNS1 to FNS4)\nTotal_counts &lt;- df %&gt;%\n  dplyr::select(c(F1:F9, FNS1:FNS4)) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", Total_count = \"Freq\")\n\n# Merge the two count tables\nfactor_scores &lt;- merge(F_counts,FNS_counts, by = \"FAC\", all = TRUE)\nfactor_scores &lt;- merge(factor_scores, Total_counts, by = \"FAC\", all = TRUE)\n\n\n# Replace NAs with 0 for cases where factors appear in some but not all sections\nfactor_scores[is.na(factor_scores)] &lt;- 0\n\nfactor_scores &lt;- factor_scores %&gt;%\n  mutate(Score_Sig = round(F_count / Total_count, 2),\n         Score_NOT_Sig = round(FNS_counts / Total_count, 2)) %&gt;% filter(FAC != \"\")\n\n\nhead(factor_scores)\n\n\n\nPart 2.2 Data Analysis\nNow let‚Äôs actually do some analysis. Let‚Äôs visualize how the themes of the papers have changed across the years. I will first generate a bar plot that fills the bars at each year (as a categorical factor) with proportions of themes in that year. This is an aggregation that happens under the hood, and using position = \"fill\" will actually make sure all the bars consider things relative to eachother, filling the full 100% of the bar.\n\nggplot(df, aes(x = as.factor(Year), fill = DecisionTheme)) +\n  geom_bar(position = \"fill\") +\n  theme_minimal() +\n  labs(fill = \"Theme\",\n       x = \"Year\",\n       y = \"Total Count\") +\n  fill_palette(\"Set3\")\n\nTo see how things move/flow over the years, a line chart is a great idea:\n\ndf %&gt;%\n    dplyr::count(DecisionTheme, Year) %&gt;%\n    ggplot(aes(x = as.factor(Year), y = n, color = DecisionTheme, group = DecisionTheme)) +\n  geom_line() +\n  geom_point() +\n  theme_minimal() +\n  labs(fill = \"Theme\",\n       x = \"Year\",\n       y = \"Total Count\") +\n  fill_palette(\"Dark2\")\n\nFor analysis, I will need to convert the data to long format. Since I want to avoid making it too big, I‚Äôll do this separately for each key variable.\n\ntheory_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = THEORY1:THEORY4,\n        names_to = \"THEORY_NAME\", \n        values_to = \"THEORY\"\n    ) \n\nmethod_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = METHOD1:METHOD4,\n        names_to = \"METHODNAME\", \n        values_to = \"METHOD\"\n    ) \n\nlimit_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = LIMIT1:LIMIT3,\n        names_to = \"LIMITNAME\", \n        values_to = \"LIMIT\"\n    ) \n\nfac_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = F1:F9,\n        names_to = \"FACNAME\",\n        values_to = \"FAC\"\n    )\n\nfac_NS_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = FNS1:FNS4,\n        names_to = \"FAC_NS_NAME\",\n        values_to = \"FAC_NS\"\n    )\n\nfactors_based_on_themes &lt;- df %&gt;% pivot_longer(\n    cols = F1_THEME:F9_THEME,\n    names_to = \"FAC_THEMES_NAMES\",\n    values_to = \"FACTHEME\"\n)\n\nRemove all the empty rows:\n\ntheory_long &lt;- theory_long %&gt;% filter(THEORY != \"\") #\nmethod_long &lt;- method_long %&gt;% filter(METHOD != \"\") #\nlimit_long &lt;- limit_long %&gt;% filter(LIMIT != \"\") #\nfac_long &lt;- fac_long %&gt;% filter(FAC != \"\")\nfac_NS_long &lt;- fac_NS_long %&gt;% filter(FAC_NS != \"\")\n\nAdd factor scores to the long factors and non-signficant factors‚Äô data:\n\nfac_long &lt;- merge(fac_long, factor_scores, by = \"FAC\", all = T)\n\n\nfactor_scores &lt;- factor_scores %&gt;% mutate(FAC_NS = FAC) %&gt;% dplyr::select(FAC_NS,Score_Sig, Score_NOT_Sig)\nfac_NS_long &lt;- merge(fac_NS_long, factor_scores, by = \"FAC_NS\", all = T)\n\n\n\nPart 2.3 Statistical Analysis\nNow, I want to explore the interactions between the following properties: themes, theories, methodologies, limitations, factors, years, research types, sample sizes, technologies, and non-significant factors. Some questions that can be answered from such an analysis are:\n\nAre there notable differences in the distribution of themes, theories, methodologies, limitations, factors, research types, sample sizes, technologies, and non-significant factors.across years?\nAre themes, theories, methodologies, limitations, factors, years, research types, sample sizes, and non-significant factors significantly associated with specific technologies?\nAre there significant differences in sample sizes across themes, theories, methodologies, limitations, factors, years, research types, technologies, and non-significant factors?\nDo research types vary significantly among different themes, theories, methodologies, limitations, factors, years, sample sizes, technologies, and non-significant factors?\nAre there significant differences in methods used across themes, theories, limitations, factors, years, research types, sample sizes, technologies, and non-significant factors?\nAre the significant factors identified notably different among themes, theories, methodologies, limitations, years, research types, sample sizes, technologies, and non-significant factors?\nAre the non-significant factors identified notably different among themes, theories, methodologies, limitations, factors, years, research types, sample sizes, and technologies?\n\nTo do this, I will first decide if further investigation is even worthwhile. First, I will use ANOVA to figure out if there are significant differences between groups of the same variable. That is, are themes, theories, methodologies, limitations, technologies, factors, years, research types, sample sizes, and non-significant factors actually different across the dataset?\n\nbuild_anova &lt;- function(nameOfCol){\n    counts_df &lt;- df %&gt;% count({{nameOfCol}}) %&gt;% arrange(desc(n))\n\n    counts_df_long &lt;- data.frame(\n        Group = rep(as.character(counts_df[[1]]), times = counts_df$n),\n        Value = unlist(lapply(counts_df$n, function(x) seq_len(x)))\n    )\n\n    anova_result &lt;- aov(Value ~ Group, data = counts_df_long)\n    return(summary(anova_result))\n}\n\nThemes are significantly different.\n\nbuild_anova(DecisionTheme)\n\nSo, let‚Äôs see how they differ across other factors - starting with the ones that do not require pivoting the dataframe! (Year, Tech, SampleSizeBin, ResearchType). This time, I will use a \\chi^2 test of independence.\n\nbuild_contingency_table &lt;- function(nameOfCol){\n    data_combine &lt;- df %&gt;% group_by(DecisionTheme) %&gt;% count({{nameOfCol}}) \n\n    contingency_table &lt;- xtabs(n ~ DecisionTheme + {{nameOfCol}}, data = data_combine)\n    chi_sq_result &lt;- chisq.test(contingency_table)\n    chi_sq_result\n}\n\n\n#build_contingency_table(SampleSizeBin)\n\nYou can also calculate the Cramer V:\n\ntable(is.na(df$SampleSizeBin))\n\n\n#cramerV(build_contingency_table(SampleSizeBin))",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR"
    ]
  },
  {
    "objectID": "study1.html#part-3.-results",
    "href": "study1.html#part-3.-results",
    "title": "The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "section": "Part 3. Results",
    "text": "Part 3. Results",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "The proliferation of Mobile banking, referred to in this project as m-banking, has transformed financial services, enabling consumers to conduct transactions conveniently through mobile devices. Despite the advantages, m-banking adoption rates vary significantly. Adoption is the early stage usage or uptake of a technology. My dissertation aims to bridge critical gaps in understanding how different factors (according to literature trends and through novel contributions) and device-specific attributes shape m-banking adoption. The central research question addressed in this dissertation is: ‚ÄúWhat are the factors that influence mobile banking adoption across different user segments?‚Äù\nThe current chapter is a holistic introduction to the dissertation. To answer the main research question, I conducted three distinct yet interconnected studies. These studies, presented in chronological order in the following chapters, are:\nTheoretical Contributions\nThis dissertation makes several contributions to the field of technology adoption and financial services research.\nPractical Contributions",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#the-inner-workings-of-mobile-banking-adoption-a-systematic-literature-review-of-intrinsic-factors",
    "href": "introduction.html#the-inner-workings-of-mobile-banking-adoption-a-systematic-literature-review-of-intrinsic-factors",
    "title": "Introduction",
    "section": "1 ### The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "text": "1 ### The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors\nDespite extensive research on m-banking adoption, a clear framework categorizing user-specific influences is still missing.\nAfter investigating the m-banking adoption literature, I found that uniform definitions for factors 1 influencing m-banking adoption was lacking. Furthermore, there was an over-reliance on only a few factors and theoretical frameworks. This prompted the undertaking of the SLR study in study 1. To address the problem of lack of uniform definitions, I categorized adoption factors based on context and impact. This way, using group/category membership help identify similarities between factors. Thus making definitions more cohesive.\nSeveral studies support the idea that categorization helps in defining and understanding concepts more easily. Research suggests that categorization plays a crucial role in defining concepts, moving from a classical view (fixed definitions) to a probabilistic view where categorization helps in making sense of concepts based on shared characteristics Medin (1989). Categorization enables more effective organization and processing of information which are essential for learning new concepts (zentallCATEGORIZATIONCONCEPTLEARNING2002?).\nTechnology adoption factors are complex and often belong to multiple categories. Because of this, studies have never explicitly categorized factors across the board. I adopt a broad approach to classifying factors, focusing on two key dimensions of decision-making based on how they relate to the user: Intrinsic and Extrinsic. In the context of m-banking adoption, intrinsic factors discuss how individuals evaluate an m-banking application (Often shortened to ‚Äúapp‚Äù) internally ‚Äî based on perceptions, goals, pressure felt from other people‚Äôs judgment, and emotions. Extrinsic factors, on the other hand, refer to the apps‚Äô measurable features, such as performance and functionality. Since extrinsic factors are easy to quantify and experienced similarly across the board, they are also straightforward to study. Thus, I focused on intrinsic factors. Because intrinsic factors relate to so many different inner processes (mentioned above), I further categorized them into four main groups:\n\nPerceptive, which are based on beliefs and perceptions.\nPersonal, which are based on individual motivations and traits.\nSocial, which are based on the impact of others on the decision-maker.\nPsychological, which are based on based on cognitive, emotional and mental processes.\n\nThis categorization provides a useful context-specific grouping. Following this, a systematic search of the m-banking literature for intrinsic factors was carried out. Scientific articles gathered were given themes using text-mining techniques ‚Äì specifically, lda for Topic Modeling. Some of these themes matched my categorization, as well. I also extended prior SLR studies by using statistical techniques ‚Äì specifically, anova ‚Äì to mathematically validate my findings. My results provide a strong empirical foundation for future researchers to do context-focused investigation.\nThe Outcome of this study is to help enhance the understanding of intrinsic factors, highlight underutilized methodologies, and identify research gaps. Additionally, I highlight the dominance of certain theoretical models while advocating for greater exploration of under-studied theories and methods. My findings show notable geographical and study-design biases, particularly longitudinal research in developed nations. My categorized framework helps scholars identify intrinsic factors, relevant theories, and research gaps, which promotes targeted future research.\nFor practitioners and policymakers, I recommend designing emotionally engaging apps, ensuring transparent risk communication, and educating users on safe practices. These steps enhance m-banking adoption and effectiveness.\n\n1.1 The Relationship Between Mental Health and Mobile Banking Adoption: Evidence from Canada\nFollowing the work of the previous study, I focused on intrinsic factors. One of the factors I found to be under-explored in the literature was mental health.\nPsychological factors are comparatively less-studied across the m-banking literature Venkatesh et al. (2012), Zou et al. (2023-05, 2023), Tiwari et al. (2021-12, 2021). I verified this further doing a quick preliminary search on Web-of-Science. Using the following search query returned 1,067 studies with no filters set on the results: !30(‚Äúmobile banking‚Äù OR ‚Äúmbanking‚Äù OR ‚Äúm-banking‚Äù) AND (‚Äúadoption‚Äù) When changing the search query to find studies specifically on psychological factors, the total number of studies returned were 157, which is about 14\\% of the total. The new search query was: !30\\parbox0.8 (‚Äúmobile banking‚Äù OR ‚Äúmbanking‚Äù OR ‚Äúm-banking‚Äù) AND \\ (‚Äúadoption‚Äù) AND (‚Äúaffective‚Äù OR ‚Äúpsychological‚Äù OR ‚Äúaffect based‚Äù OR ‚Äúaffect-based‚Äù OR ‚Äúemotional‚Äù OR ‚Äúcognitive‚Äù OR ‚Äúmental‚Äù OR ‚Äúpersonal‚Äù) ¬†\\ Using the same search queries in Scopus, 753 results were returned for the first, and 101 in the second search (13.4\\%). It is clear that only a small portion of the literature in m-banking adoption is focused on psychological factors.\nIn this study, I investigated the direct and moderated effect of mh on m-banking adoption. Moderators were extracted from theories in technology adoption or from literature related to mental health: rs, sd, and sns. I used a fixed effect logistic regression model grouped based on Canadian provinces following the cluster sampling design of my dataset. The results showed that mental health significantly and negatively affects m-banking adoption: better mental health outcomes were associated with lower likelihood of m-banking adoption. I did not find sufficient evidence for the moderating effects.\n%‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî- Device Divide: Unpacking Mobile Banking Adoption Differences for Smartphones and Smart Wearable Devices -section-p3 %‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî- Moving on from focusing only on smartphones, I decided to do a comparative study considering newer devices used in m-banking. One of the findings of my SLR study from Section -section-p2 was that few comparative studies exist in general, and studies that focus on various tools used for m-banking are increasingly important. In this chapter, I examined the nuances of m-banking adoption across two mobile devices: smartphones and smart wearable devices. I investigated the impact of the following factors: trust, perceived security, perceived value (time savings), and demographic variables. Demographic factors are important as numerous studies identify these (e.g., age, gender, income, and education) as key factors influencing m-banking adoption Chawla & Joshi (2018), lyInternetBankingAdoption2022e.\nThe results from this chapter are device-specific insights. This study also refines existing technology adoption models. Additionally, to the best of my knowledge, this is the first study to compare behavioral differences between smartphone and smart wearable users in the context of m-banking. I found that trust impacts smartphone users more strongly, while wearable users prioritize time efficiency. Users perceived smartphones as more secure. Demographic factors such as age, education, and gender exhibited varying influences based on device type as well.\n\n\n1.2 References"
  },
  {
    "objectID": "introduction.html#footnotes",
    "href": "introduction.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe words factor and construct are used interchangeably throughout most of m-banking adoption literature with no significant distinction between them Oyetade et al. (2020).‚Ü©Ô∏é",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#the-relationship-between-mental-health-and-mobile-banking-adoption-evidence-from-canada",
    "href": "introduction.html#the-relationship-between-mental-health-and-mobile-banking-adoption-evidence-from-canada",
    "title": "Introduction",
    "section": "2 ### The Relationship Between Mental Health and Mobile Banking Adoption: Evidence from Canada",
    "text": "2 ### The Relationship Between Mental Health and Mobile Banking Adoption: Evidence from Canada\nFollowing the work of the previous study, I focused on intrinsic factors. One of the factors I found to be under-explored in the literature was mental health.\nPsychological factors are comparatively less-studied across the m-banking literature Venkatesh et al. (2012), Zou et al. (2023-05, 2023), Tiwari et al. (2021-12, 2021). I verified this further doing a quick preliminary search on Web-of-Science. Using the following search query returned 1,067 studies with no filters set on the results: !30(‚Äúmobile banking‚Äù OR ‚Äúmbanking‚Äù OR ‚Äúm-banking‚Äù) AND (‚Äúadoption‚Äù) When changing the search query to find studies specifically on psychological factors, the total number of studies returned were 157, which is about 14\\% of the total. The new search query was: !30\\parbox0.8 (‚Äúmobile banking‚Äù OR ‚Äúmbanking‚Äù OR ‚Äúm-banking‚Äù) AND \\ (‚Äúadoption‚Äù) AND (‚Äúaffective‚Äù OR ‚Äúpsychological‚Äù OR ‚Äúaffect based‚Äù OR ‚Äúaffect-based‚Äù OR ‚Äúemotional‚Äù OR ‚Äúcognitive‚Äù OR ‚Äúmental‚Äù OR ‚Äúpersonal‚Äù) ¬†\\ Using the same search queries in Scopus, 753 results were returned for the first, and 101 in the second search (13.4\\%). It is clear that only a small portion of the literature in m-banking adoption is focused on psychological factors.\nIn this study, I investigated the direct and moderated effect of mh on m-banking adoption. Moderators were extracted from theories in technology adoption or from literature related to mental health: rs, sd, and sns. I used a fixed effect logistic regression model grouped based on Canadian provinces following the cluster sampling design of my dataset. The results showed that mental health significantly and negatively affects m-banking adoption: better mental health outcomes were associated with lower likelihood of m-banking adoption. I did not find sufficient evidence for the moderating effects.\n%‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî- Device Divide: Unpacking Mobile Banking Adoption Differences for Smartphones and Smart Wearable Devices -section-p3 %‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî- Moving on from focusing only on smartphones, I decided to do a comparative study considering newer devices used in m-banking. One of the findings of my SLR study from Section -section-p2 was that few comparative studies exist in general, and studies that focus on various tools used for m-banking are increasingly important. In this chapter, I examined the nuances of m-banking adoption across two mobile devices: smartphones and smart wearable devices. I investigated the impact of the following factors: trust, perceived security, perceived value (time savings), and demographic variables. Demographic factors are important as numerous studies identify these (e.g., age, gender, income, and education) as key factors influencing m-banking adoption Chawla & Joshi (2018), lyInternetBankingAdoption2022e.\nThe results from this chapter are device-specific insights. This study also refines existing technology adoption models. Additionally, to the best of my knowledge, this is the first study to compare behavioral differences between smartphone and smart wearable users in the context of m-banking. I found that trust impacts smartphone users more strongly, while wearable users prioritize time efficiency. Users perceived smartphones as more secure. Demographic factors such as age, education, and gender exhibited varying influences based on device type as well.\n\n2.1 References"
  },
  {
    "objectID": "introduction.html#the-device-divide-unpacking-mobile-banking-adoption-differences-for-smartphones-and-smart-wearable-devices-labelintro-section-p3",
    "href": "introduction.html#the-device-divide-unpacking-mobile-banking-adoption-differences-for-smartphones-and-smart-wearable-devices-labelintro-section-p3",
    "title": "Introduction",
    "section": "1 The Device Divide: Unpacking Mobile Banking Adoption Differences for Smartphones and Smart Wearable Devices ‚Äúlabelintro-section-p3",
    "text": "1 The Device Divide: Unpacking Mobile Banking Adoption Differences for Smartphones and Smart Wearable Devices ‚Äúlabelintro-section-p3\n%‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî- Moving on from focusing only on smartphones, I decided to do a comparative study considering newer devices used in m-banking. One of the findings of my SLR study from Section ‚Äúrefintro-section-p2 was that few comparative studies exist in general, and studies that focus on various tools used for m-banking are increasingly important. In this chapter, I examined the nuances of m-banking adoption across two mobile devices: smartphones and smart wearable devices. I investigated the impact of the following factors: trust, perceived security, perceived value (time savings), and demographic variables. Demographic factors are important as numerous studies identify these (e.g., age, gender, income, and education) as key factors influencing m-banking adoption Chawla & Joshi (2018), lyInternetBankingAdoption2022e.\nThe results from this chapter are device-specific insights. This study also refines existing technology adoption models. Additionally, to the best of my knowledge, this is the first study to compare behavioral differences between smartphone and smart wearable users in the context of m-banking. I found that trust impacts smartphone users more strongly, while wearable users prioritize time efficiency. Users perceived smartphones as more secure. Demographic factors such as age, education, and gender exhibited varying influences based on device type as well.\n\n1.1 References"
  },
  {
    "objectID": "study1.html",
    "href": "study1.html",
    "title": "The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "section": "",
    "text": "I downloaded the pdf of all the papers (143), reading them and extracting meta data based on the following:\n\nimport numpy as np \n\ndatabase = np.array([\n    {\n        'id': 'string', # unique identifier for the paper following convention P2_#number \n        'title': 'string', # title of the paper\n        'AffiliationCountry': 'string' , #name of country the study was conducted in,\n        'year': 2018-2024, # year of publication a value between 2018 and 2024\n        'journal': 'string', # name of the journal the paper was published in\n        'citations': 0-1000, # number of citations the paper has received - not reported in the paper \n        'year_since': 3, # number of years since publication - not reported in the paper \n        'cpy': 0, # number of citations per year - not reported in the paper \n        'keywords': ['TAM', 'mbanking', 'awareness'], # list of keywords, broken into K1-K10\n        'abstract': 'string', # abstract of the paper \n        'F': ['perceived usefulness'], # factors significant in the study, broken into F1-F9 \n        'FN': ['another factor'], # factors not significant in the study, broken into FNS1-FNS4 \n        'limit': ['geographical context'], # limitations of the study, broken into LIMIT1-LIMIT3 \n        'typeofResearch': 'string', # type of research conducted in the study \n        'methods': ['regression analysis'], # methods used in the study, broken into METHOD1-METHOD4\n        'theory': ['TAM'] # theories used in the study, broken into THEORY1-THEORY4\n        'sampleSize': 100, # sample size of the study \n        'tech': 'string', # main technology studied \n        'man_theme': 'string', # Theme manually assigned by me \n        'algo_theme': 'string', # Theme assigned by the algorithm \n        'decision_Theme': 'string', # Final theme of the paper  \n        'Score_Sig': 0.0, # % of significance for factors \n        'Score_NOT_Sig': 0.0, # % of non-significance for factors\n    }\n])\n\n\n\nIdea for future\n\nü§ñ Build an Agentic AI application that automates this process.\n\n\n\nFirst, install the following Python modules:\n\n%%capture \n!pip install nltk\n!pip install gensim\n!pip install itertools\n!pip install spacy\n!pip install langdetect\n!pip install pprint\n!pip install pyLDAvis\n!pip install textract\n!pip install spacy\n!pip install pymupdf\n!pip uninstall matplotlib seaborn -y\n!pip install matplotlib seaborn  \n!pip install --upgrade matplotlib seaborn\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n!pip install scipy==1.12.0 --quiet\n\nSince I am not familiar with Docker, I couldn‚Äôt resolve the package dependencies. This took so much time for me and I finally managed to fix it with this specific configuration. The imports look scary:\n\nimport string\nimport os \nimport re # regular expression \nimport pandas as pd\nimport numpy as np\n__requires__= 'scipy==1.12.0'\nimport scipy \nimport itertools\nimport textract # PDF text extraction \nimport math\nimport spacy\nimport fitz #PyMuPDF - another (better) PDF text extraction \n\n#NLP imports\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.util import ngrams\nfrom nltk.tokenize import RegexpTokenizer\n# from nltk import pos_tag # didn't actually use it \n\n#SKLEARN\nfrom sklearn import metrics\nfrom sklearn import neighbors\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\n# from sklearn.decomposition import PCA\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.metrics import classification_report\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.neighbors import NearestNeighbors\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n# from sklearn.naive_bayes import (\n# BernoulliNB,\n# ComplementNB,\n# MultinomialNB,\n# )\n#from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.svm import SVC\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.neural_network import MLPClassifier\n# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics.pairwise import cosine_similarity\n#from sklearn.decomposition import LatentDirichletAllocation\n\n#GENSIMimports\nimport gensim\nfrom gensim.models import Phrases\nfrom gensim.models.phrases import Phraser\nfrom gensim.corpora.dictionary import Dictionary\nfrom gensim.corpora import MmCorpus\nfrom gensim.models.tfidfmodel import TfidfModel\nfrom gensim.models import CoherenceModel\nfrom gensim.models import KeyedVectors\n\n#PyLDAvis imports for visualization of topic modeling results \n# import pyLDAvis\n# import pyLDAvis.gensim_models as gensimvis\n# import pyLDAvis.gensim\n# import pyLDAvis.gensim_models\n\n#MISC imports\nfrom collections import Counter\nfrom collections import defaultdict\nfrom string import punctuation\nfrom pprint import pprint\nfrom numpy import triu\n#from scipy.linalg.special_matrices import triu\nfrom scipy.sparse import csr_matrix\n\n#TRANSFORMERS\n#import torch\n#importtensorflowastf\n#from transformers import BertTokenizer, BertModel\n#from transformers import AutoTokenizer, AutoModel\n#fromtensorflow.keras.modelsimportSequential\n#fromtensorflow.keras.preprocessing.textimportTokenizer\n#fromtensorflow.keras.preprocessing.sequenceimportpad_sequences\n#fromtensorflow.keras.layersimportDense,Embedding,LSTM,SpatialDropout1D\n#fromtensorflow.keras.layersimportLeakyReLU\n\n#MATPLOT\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nDownload some of the language support stuff:\n\n# only run once\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('punkt_tab')\nnltk.download('omw-1.4')  # Optional \n#nltk.download('averaged_perceptron_tagger')  # For POS tagging\n#nltk.download('averaged_perceptron_tagger_eng') # POS tagging \n\nI saved the pdf files‚Äô name in a dictionary like this:\n\nname_of_pdfs = {\n    'p2_01': \"Lonkani et al_2020_A comparative study of trust in mobile banking.pdf\", \n    'p2_02': \"Saprikis et al_2022_A comparative study of users versus non-users' behavioral intention towards.pdf\", \n    'p2_03': \"Malaquias et al_2021_A cross-country study on intention to use mobile banking.pdf\", \n    'p2_04': \"Merhi et al_2019_A cross-cultural study of the intention to use mobile banking between Lebanese.pdf\", \n    'p2_05': \"Frimpong et al. - 2020 - A cross‚Äênational investigation of trait antecedent.pdf\", \n    # and so on ... \n}\n\nAdditionally, I defined a dictionary ‚Äúlook up‚Äù for all the factors in the dataset with their related theme that looks like this (shortened for this presentation):\n\ntheme_of_words = {\n    'demographic': \n        list(set(['women', 'woman', 'female', 'men', 'man', 'male', 'sex', 'gender', 'age', 'income', \n            'demographic variables', 'elderly', 'education', 'gender differences', 'generation y', 'millennial generation',\n            'millennial', 'gen y', 'gen Z', 'gen alpha', 'gen X', 'boomer', 'babyboomer', 'generation X', 'generation z',\n            'young consumers', \n            # A lot more factors ...\n            ])),\n    \n    #----------------------------------------------------------------------------------------------------------------------------------\n    'cultural': \n        list(set(['developing countries','malaysia','transition country','pakistan',\n            'zakat','developing country','ghana','USA','srilanka', 'sri lanka',\n            'india','maldives','saudi-arabia','saudi arabia', 'nigeria','thailand','united states',\n            'yemen','citizenship','zimbabwe','palestine','culture',\n            'Country perspective', \n            # ... \n            ])),\n    \n    #----------------------------------------------------------------------------------------------------------------------------------\n    'psychological':\n        list(set(['anxiety','satisfaction','behavior','behaviour','attitudes','attitude','awareness',\n            'technology anxiety','consumer-behavior','trust','benv','consumer behaviour',\n            'covid-19 related psychological distress','psychological distress','psychological','distress',\n            'behavioral','computer anxiety','customer satisfaction', 'cognitive resistance',\n            # A LOT more ... \n            ]))\n            , \n            # ... few other key value pairs corresponding to themes \n\n}\n\nI also needed to delete some stop words, and decided to add more words that I knew would be frequently repeated. I also define the lemmer and stemmer.\n\nstop_words = stopwords.words('english')\nstop_words.extend([\"bank\", \"banking\", \"banks\", \n                   \"mobile\", \"mbank\", \"mbanking\", \"m-bank\", \"m bank\",\n                   \"online\", \"e\", \"e-bank\", \"ebank\", \"mobile banking\", \"mobile bank\", \n                   \"adoption\", \"acceptance\", \"accept\", \"theory\", \n                   \"purpose\", \"result\", \"method\", #from abstracts \n                   \"journal\", \"volume\", \"pp\", \"no\", \"doi\", \"http\", \"https\", \"et al\", \"issue\",\n                   \"technology\", \"internet\", \"information system\", \"international information\",\n                   \"information technology\", \"computer human\", \"mis quarterly\", \"electornic commerce\",\n                   \"j market\", \"telematics and informatics\", \"telematics informatics\", \"retail consumer\",\n                   \"international volume\", \"international business\", \"global information\",\n                   \"et\", \"al\", \"al.\", \"tam\", \"sem\", \"pls\", \"utaut\", \"tpb\",\n                   \".com\", \"management\", \"marketing\", \"published\", \"study\",\n                   \"research\", \"literature\", \"model\", #from journal information \n                   \"app\", \"application\", \"usage\"])\n\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\n\n\nSo, I need a few functions as set up for cleaning the text. Function extract_text_from_pdf() is using PyMuPDF to extract text from a PDF file.\n\n#version one using PyMuPDF \ndef extract_text_from_pdf(filename):\n    text = \"\"\n    try:\n        doc = fitz.open(filename)\n        for page_num in range(doc.page_count):\n            page = doc.load_page(page_num)\n            text += page.get_text()\n    except Exception as e:\n        print(f\"Error reading {filename}: {e}\")\n    return text\n\nThis function is just one of the data cleaning functions:\n\ndef preprocess_Dict(dct):\n    for k, v in dct.items():\n        if isinstance(v, list):\n            processed_list = []\n            for item in v:\n                item = item.lower()\n                item = re.sub(r'http\\S+|www\\S+|@\\S+', '', item)\n                item = re.sub(r'(?&lt;=\\w)-(?=\\w)', ' ', item)\n                item = re.sub(r'[^a-z0-9\\s\\n]', '', item)\n                item = re.sub(r'\\s+', ' ', item).strip()\n                item = re.sub(r'\\d+', '', item).strip()\n\n                # replacing abbreviations \n                item = item.replace('structural equation model', 'sem')\n                item = item.replace('technology acceptance model', 'tam')\n                item = item.replace('unified theory of acceptance and use of technology', 'utaut')\n                item = item.replace('diffusion of innovation', 'doi')\n                item = item.replace('partial least squares', 'pls')\n                item = item.replace('theory of planned behavior', 'tpb')\n                item = re.sub(\"perceived usefulness\", \"pu\", item)\n                item = re.sub(\"perceived ease of use\", \"peou\", item)\n                item = re.sub(\"perceived privacy\", \"priv\", item)\n                item = re.sub(\"perceived aesthetics\", \"p_aest\", item)\n                item = re.sub(\"perceived relative advantage\", \"p_rel_adv\", item)\n                item = re.sub(\"perceived risk\", \"prisk\", item)\n                item = re.sub(\"perceived enjoyment\",\"penjy\", item)\n                item = re.sub(\"perceived intelligence\",\"pintlj\", item)\n                item = re.sub(\"perceived security\",\"psec\", item)\n                item = re.sub(\"perceived trust\",\"ptrst\", item)\n                item = re.sub(\"perceived anthropomorphism\",\"panthro\", item)\n                item = re.sub(\"perceived value\",\"pval\", item)\n                item = re.sub(\"perceived compatibility\",\"pcompat\", item)\n                item = re.sub(\"perceived detterants\",\"pdet\", item)\n                item = re.sub(\"perceived behavioral control\", \"p_bhv_ctrl\", item)\n                item = re.sub(\"perceived credibility\",\"pcred\", item)\n                item = re.sub(\"perceived cost\",\"pcost\", item)\n                item = re.sub(\"perceived benefit\",\"pbenef\", item)\n                item = re.sub(\"perceived convenience\",\"pconv\", item)\n                item = re.sub(\"perceived usability\",\"pusbl\", item)\n                item = re.sub(\"perceived privacy concerns\", \"ppriv_cn\", item)\n                \n                # belief base \n                item = re.sub(\"performance expectancy\", \"peex\", item)\n                item = re.sub(\"convenience\", \"conv\", item)\n                item = re.sub(\"effort expectancy\",\"efex\", item)\n                item = re.sub(\"access convenience\",\"acc_conv\", item)\n                item = re.sub(\"reliability\", \"rely\", item)\n                item = re.sub(\"behavioral control\", \"bhv_ctrl\", item)\n                item = re.sub(\"compatibility\", \"compat\", item)\n                item = re.sub(\"normative beliefs\", \"norm_blf\", item)\n                item = re.sub(\"normative belief\", \"norm_blf\", item)\n                item = re.sub(\"transaction convenience\",\"trans_conv\", item)\n                item = re.sub(\"post use trust\", \"post_trst\", item)\n                item = re.sub(\"post-use trust\", \"post_trst\", item)\n                item = re.sub(\"benefit convenience\",\"ben_conv\", item)\n                item = re.sub(\"search convenience\", \"srch_conv\", item)\n                item = re.sub(\"utilitarian expectation\", \"util_exp\", item)\n                item = re.sub(\"evaluation convenience\", \"eval_conv\", item)\n                item = re.sub(\"expectation\", \"expect\", item)\n                item = re.sub(\"possession convenience\", \"poss_conv\", item)\n                item = re.sub(\"expected advantage\", \"exp_adv\", item)\n               \n                # intention \n                item = re.sub(\"intention\", \"intnt\", item)\n                item = re.sub(\"motivation\", \"motiv\", item)\n                item = re.sub(\"automative motivation\",\"auto_motiv\", item)\n                item = re.sub(\"behavioral intention\",\"bhv_intnt\", item)\n                item = re.sub(\"control motivation\",\"ctrl_motiv\", item)\n                item = re.sub(\"controlled motivation\", \"ctrl_motiv\",  item)\n                item = re.sub(\"hedonic motivation\",\"hed_motiv\", item)\n                item = re.sub(\"intention to use\",\"intnt_use\", item)\n                \n                # personal \n                item = re.sub(\"habit\", \"habt\", item)\n                item = re.sub(\"personality\",\"prsnl\",item)\n                item = re.sub(\"personal factors\",\"prsnl\",item)\n                item = re.sub(\"personal factor\", \"prsnl\", item)\n                item = re.sub(\"digital literacy\",\"dig_lit\",item)\n                item = re.sub(\"digital capability\",\"dig_cabl\",item)\n                item = re.sub(\"agreeableness\",\"agrbns\",item)\n                item = re.sub(\"financial literacy\",\"fin_lit\",item)\n                item = re.sub(\"previous experience\",\"prv_exp\",item)\n                item = re.sub(\"life compatibility\",\"life_compat\",item)\n                item = re.sub(\"lifestyle\", \"life\", item)\n                item = re.sub(\"knowledge\",\"know\",item)\n                item = re.sub(\"functional value\",\"fun_val\",item)\n                item = re.sub(\"fun value\", \"fun_val\", item)\n                item = re.sub(\"utalitarian value\",\"util_val\",item)\n                item = re.sub(\"epistemic value\",\"epi_val\",item)\n                item = re.sub(\"monetary value\",\"mon_val\",item)\n                item = re.sub(\"money value\",\"mon_val\",item)\n                item = re.sub(\"hedonic value\", \"hed_val\", item)\n                item = re.sub(\"emotional value\",\"emo_val\",item)\n                item = re.sub(\"quality value\",\"qual_val\",item)\n                item = re.sub(\"value barriers\", \"val_bar\", item)\n                item = re.sub(\"value barrier\",\"val_bar\",item)\n                item = re.sub(\"customer experience about usability\",\"exp_use\",item)\n                item = re.sub(\"experience\",\"exp\",item)\n                item = re.sub(\"self employment\",\"semp\",item)\n                item = re.sub(\"self-employment\",\"semp\",item)\n                item = re.sub(\"valence\",\"val\",item)\n                item = re.sub(\"religiosity\",\"religis\",item)\n                item = re.sub(\"task technology fit\",\"ttf\",item)\n                item = re.sub(\"lifestyle fit\",\"life_fit\",item)\n                    \n                # social \n                item = re.sub(\"social interactions on platforms\",\"soc_int_plt\", item)\n                item = re.sub(\"coercive pressures\", \"coe_prsr\",  item)\n                item = re.sub(\"coercive pressure\", \"coe_prsr\",  item)\n                item = re.sub(\"human human interaction\",\"hh_int\", item)\n                item = re.sub(\"human-human interaction\",\"hh_int\", item)\n                item = re.sub(\"social influence\", \"socinf\", item)\n                item = re.sub(\"collectivist cultural practices\", \"colcul\",  item)\n                item = re.sub(\"collectivist cultural practice\",\"colcul\", item)\n                item = re.sub(\"social media influence\",\"snsinf\", item)\n                item = re.sub(\"normative belief\",\"norm_blf\", item)\n                item = re.sub(\"interaction\",\"interac\", item)\n                item = re.sub(\"subjective norm\", \"sbj_nrm\",  item)\n                item = re.sub(\"subjective norms\", \"sbj_nrm\",  item)\n                item = re.sub(\"social factors\", \"soc_fac\",  item)\n                item = re.sub(\"social factor\" ,\"soc_fac\" , item)\n                item = re.sub(\"normative pressure\",\"nrm_prsr\", item)\n                item = re.sub(\"CSR economical responsibility\",\"csr_econ\", item)\n                item = re.sub(\"social norms\", \"soc_nrm\",  item)\n                item = re.sub(\"family influence\",\"fam_inf\", item)\n                item = re.sub(\"people\",\"people\", item)\n                item = re.sub(\"herd\",\"herd\", item)\n                item = re.sub(\"CSR social responsibility\",\"csr_soc\", item)\n                item = re.sub(\"mimetic pressure\", \"mim_prsr\",  item)\n                item = re.sub(\"CSR environmental responsibility\", \"csr_env\",  item)\n                item = re.sub(\"social value\",\"soc_val\", item)\n                item = re.sub(\"social values\",\"soc_val\", item)\n                item = re.sub(\"employee customer engagement\",\"engg_empcus\", item)\n                item = re.sub(\"employee-customer engagement\",\"engg_empcus\", item)\n                item = re.sub(\"social isolation\", \"soc_iso\",  item)\n                item = re.sub(\"normative pressures\",\"norm_prsr\", item)\n                item = re.sub(\"social proof social media\", \"soc_prf_sns\",  item)\n                item = re.sub(\"social proof\", \"soc_prf\",  item)\n                item = re.sub(\"social media\",\"sns\", item)\n                item = re.sub(\"word of mouth\", \"wom\",  item)\n                item = re.sub(\"wom\",\"wom\", item)\n                item = re.sub(\"word-of-mouth\", \"wom\",  item)\n                item = re.sub(\"w-o-m\", \"wom\",  item)\n                item = re.sub(\"wordmouth\", \"wom\",  item)\n                item = re.sub(\"environment\", \"env\",  item)\n                \n                \n                # psychological \n                item = re.sub(\"computer self efficacy\",\"self\",item)\n                item = re.sub(\"self efficacy\",\"self\",item)\n                item = re.sub(\"self-efficacy\", \"self\", item)\n                item = re.sub(\"attitude\", \"attd\", item)\n                item = re.sub(\"attitudes\",\"attd\",item)\n                item = re.sub(\"trust\",\"trst\",item)\n                item = re.sub(\"pragmatic\",\"prgt\",item)\n                item = re.sub(\"security concern\", \"sec_cn\",item)\n                item = re.sub(\"security concerns\",\"sec_cn\",item)\n                item = re.sub(\"self-image\", \"self_cong\", item)\n                item = re.sub(\"self image\",\"self_cong\",item)\n                item = re.sub(\"congruence\", \"cong\", item)\n                item = re.sub(\"self-congruence\",\"self_cong\",item)\n                item = re.sub(\"self congruence\", \"self_cong\", item)\n                item = re.sub(\"self-image congruence\",\"self_cong\",item)\n                item = re.sub(\"self image congruence\", \"self_cong\", item)\n                item = re.sub(\"selfimage congruence\", \"self_cong\", item)\n                item = re.sub(\"awareness\", \"awar\", item)\n                item = re.sub(\"satisfaction\",\"satis\",item)\n                item = re.sub(\"consumer satisfaction\",\"satis\",item)\n                item = re.sub(\"customer satisfaction\",\"satis\",item)\n                item = re.sub(\"restiant to change\",\"resist_chng\",item)\n                item = re.sub(\"resistance to change\",\"resist_chng\",item)\n                item = re.sub(\"risk aversion\", \"risk_avrs\", item)\n                item = re.sub(\"risk averse\",\"risk_avrs\",item)\n                item = re.sub(\"novelty\",\"new_seek\",item)\n                item = re.sub(\"novelty-seeking\", \"new_seek\", item)\n                item = re.sub(\"novelty seeking\", \"new_seek\", item)\n                item = re.sub(\"consciousnesnness\", \"conscn\", item)\n                item = re.sub(\"post-use trust\", \"post_trst\", item)\n                item = re.sub(\"post use trust\",\"post_trst\",item)\n                item = re.sub(\"postuse trust\",\"post_trst\",item)\n                item = re.sub(\"emotional experience\",\"emo_exp\",item)\n                item = re.sub(\"agreeableness\",\"agrbns\",item)\n                item = re.sub(\"privacy concerns\",\"priv_cn\",item)\n                item = re.sub(\"privacy concern\",\"priv_cn\",item)\n                item = re.sub(\"cognitive decline\",\"cog_dec\",item)\n                item = re.sub(\"benevolent convenince\",\"ben_conv\",item)\n                item = re.sub(\"enjoyment\",\"enjy\",item)\n                item = re.sub(\"enjoy\", \"enjy\", item)\n                item = re.sub(\"hedonic motivation\",\"hed_motiv\",item)\n                item = re.sub(\"oppenness\", \"open\", item)\n                item = re.sub(\"loyal\", \"loyal\", item)\n                item = re.sub(\"loyalty\",\"loyal\",item)\n                item = re.sub(\"confirmation\",\"confrm\",item)\n                item = re.sub(\"optimism\",\"optim\",item)\n                item = re.sub(\"safety concerns\",\"safe_cn\",item)\n                item = re.sub(\"safety concern\",\"safe_cn\",item)\n                item = re.sub(\"Covid-19 psychological distress\", \"dist_covid\", item)\n                item = re.sub(\"Covid19 psychological distress\", \"dist_covid\", item)\n                item = re.sub(\"Covid 19 psychological distress\",\"dist_covid\",item)\n                item = re.sub(\"psychological distress\",\"dist_covid\",item)\n                item = re.sub(\"green concerns\",\"green_cn\",item)\n                item = re.sub(\"technology anxiety\", \"anxiety\", item)\n                item = re.sub(\"anxiety\",\"anxiety\",item)\n                item = re.sub(\"obedience\", \"obed\", item)\n                item = re.sub(\"empathy\",\"empath\",item)\n                item = re.sub(\"decision comfort\",\"dec_comfrt\",item)\n                item = re.sub(\"confidence\",\"confdnc\",item)\n                item = re.sub(\"decision discomfort\", \"dec_dis_comfrt\", item)\n                item = re.sub(\"comfort\", \"cmfrt\", item)\n                item = re.sub(\"discomfort\",\"discmfrt\",item)\n                item = re.sub(\"insecurity\", \"insec\", item)\n                item = re.sub(\"insecurities\", \"insec\", item)\n                item = re.sub(\"benevolence\",\"benv\",item)\n                item = re.sub(\"technology stress\",\"tech_strss\",item)\n                item = re.sub(\"stress\", \"tech_strss\", item)\n                item = re.sub(\"techno-stress\",\"tech_strss\",item)\n                item = re.sub(\"technostress\",\"tech_strss\",item)\n                item = re.sub(\"techno stress\", \"tech_strss\", item)\n                item = re.sub(\"cognitive resistence\",\"cog_resist\",item)\n                        \n                # demographic \n                item = re.sub(\"age\",\"age\",item)\n                item = re.sub(\"sex\",\"sex\",item)\n                item = re.sub(\"education\",\"edu\",item)\n                item = re.sub(\"income\",\"income\",item)\n                item = re.sub(\"islamic religiosity\",\"religios\",item)\n                item = re.sub(\"culture\",\"cltr\",item)\n                \n                item = \" \".join([word for word in item.split() if word not in stop_words])\n                item = \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in item.split()])\n                #item = \" \".join([stemmer.stem(word) for word in item.split()])\n                processed_list.append(item)\n            dct[k] = \" \".join(processed_list)\n        else:\n            v = v.lower()\n            v = re.sub(r'http\\S+|www\\S+|@\\S+', '', v)\n            v = re.sub(r'(?&lt;=\\w)-(?=\\w)', ' ', v)\n            v = re.sub(r'[^a-z0-9\\s\\n]', '', v)\n            v = re.sub(r'\\s+', ' ', v).strip()\n            v = re.sub(r'\\d+', '', v).strip()\n\n            # replacing abbreviations \n            v = v.replace('structural equation model', 'sem')\n            v = v.replace('technology acceptance model', 'tam')\n            v = v.replace('unified theory of acceptance and use of technology', 'utaut')\n            v = v.replace('diffusion of innovation', 'doi')\n            v = v.replace('partial least squares', 'pls')\n            v = v.replace('theory of planned behavior', 'tpb')\n            v = re.sub(\"perceived usefulness\", \"pu\", v)\n            v = re.sub(\"perceived ease of use\", \"peou\", v)\n            v = re.sub(\"perceived privacy\", \"priv\", v)\n            v = re.sub(\"perceived aesthetics\", \"p_aest\", v)\n            v = re.sub(\"perceived relative advantage\", \"p_rel_adv\", v)\n            v = re.sub(\"perceived risk\", \"prisk\", v)\n            v = re.sub(\"perceived enjoyment\",\"penjy\", v)\n            v = re.sub(\"perceived intelligence\",\"pintlj\", v)\n            v = re.sub(\"perceived security\",\"psec\", v)\n            v = re.sub(\"perceived trust\",\"ptrst\", v)\n            v = re.sub(\"perceived anthropomorphism\",\"panthro\", v)\n            v = re.sub(\"perceived value\",\"pval\", v)\n            v = re.sub(\"perceived compatibility\",\"pcompat\", v)\n            v = re.sub(\"perceived detterants\",\"pdet\", v)\n            v = re.sub(\"perceived behavioral control\", \"p_bhv_ctrl\", v)\n            v = re.sub(\"perceived credibility\",\"pcred\", v)\n            v = re.sub(\"perceived cost\",\"pcost\", v)\n            v = re.sub(\"perceived benefit\",\"pbenef\", v)\n            v = re.sub(\"perceived convenience\",\"pconv\", v)\n            v = re.sub(\"perceived usability\",\"pusbl\", v)\n            v = re.sub(\"perceived privacy concerns\", \"ppriv_cn\", v)\n                \n            v = re.sub(\"performance expectancy\", \"peex\", v)\n            v = re.sub(\"convenience\", \"conv\", v)\n            v = re.sub(\"effort expectancy\",\"efex\", v)\n            v = re.sub(\"access convenience\",\"acc_conv\", v)\n            v = re.sub(\"reliability\", \"rely\", v)\n            v = re.sub(\"behavioral control\", \"bhv_ctrl\", v)\n            v = re.sub(\"compatibility\", \"compat\", v)\n            v = re.sub(\"normative beliefs\", \"norm_blf\", v)\n            v = re.sub(\"normative belief\", \"norm_blf\", v)\n            v = re.sub(\"transaction convenience\",\"trans_conv\", v)\n            v = re.sub(\"post use trust\", \"post_trst\", v)\n            v = re.sub(\"post-use trust\", \"post_trst\", v)\n            v = re.sub(\"benefit convenience\",\"ben_conv\", v)\n            v = re.sub(\"search convenience\", \"srch_conv\", v)\n            v = re.sub(\"utilitarian expectation\", \"util_exp\", v)\n            v = re.sub(\"evaluation convenience\", \"eval_conv\", v)\n            v = re.sub(\"expectation\", \"expect\", v)\n            v = re.sub(\"possession convenience\", \"poss_conv\", v)\n            v = re.sub(\"expected advantage\", \"exp_adv\", v)\n            \n            # intention \n            v = re.sub(\"intention\", \"intnt\", v)\n            v = re.sub(\"motivation\", \"motiv\", v)\n            v = re.sub(\"automative motivation\",\"auto_motiv\", v)\n            v = re.sub(\"behavioral intention\",\"bhv_intnt\", v)\n            v = re.sub(\"control motivation\",\"ctrl_motiv\", v)\n            v = re.sub(\"controlled motivation\", \"ctrl_motiv\",  v)\n            v = re.sub(\"hedonic motivation\",\"hed_motiv\", v)\n            v = re.sub(\"intention to use\",\"intnt_use\", v)\n                \n                # personal \n            v = re.sub(\"habit\", \"habt\", v)\n            v = re.sub(\"personality\",\"prsnl\",v)\n            v = re.sub(\"personal factors\",\"prsnl\",v)\n            v = re.sub(\"personal factor\", \"prsnl\", v)\n            v = re.sub(\"digital literacy\",\"dig_lit\",v)\n            v = re.sub(\"digital capability\",\"dig_cabl\",v)\n            v = re.sub(\"agreeableness\",\"agrbns\",v)\n            v = re.sub(\"financial literacy\",\"fin_lit\",v)\n            v = re.sub(\"previous experience\",\"prv_exp\",v)\n            v = re.sub(\"life compatibility\",\"life_compat\",v)\n            v = re.sub(\"lifestyle\", \"life\", v)\n            v = re.sub(\"knowledge\",\"know\",v)\n            v = re.sub(\"functional value\",\"fun_val\",v)\n            v = re.sub(\"fun value\", \"fun_val\", v)\n            v = re.sub(\"utalitarian value\",\"util_val\",v)\n            v = re.sub(\"epistemic value\",\"epi_val\",v)\n            v = re.sub(\"monetary value\",\"mon_val\",v)\n            v = re.sub(\"money value\",\"mon_val\",v)\n            v = re.sub(\"hedonic value\", \"hed_val\", v)\n            v = re.sub(\"emotional value\",\"emo_val\",v)\n            v = re.sub(\"quality value\",\"qual_val\",v)\n            v = re.sub(\"value barriers\", \"val_bar\", v)\n            v = re.sub(\"value barrier\",\"val_bar\",v)\n            v = re.sub(\"customer experience about usability\",\"exp_use\",v)\n            v = re.sub(\"experience\",\"exp\",v)\n            v = re.sub(\"self employment\",\"semp\",v)\n            v = re.sub(\"self-employment\",\"semp\",v)\n            v = re.sub(\"valence\",\"val\",v)\n            v = re.sub(\"religiosity\",\"religis\",v)\n            v = re.sub(\"task technology fit\",\"ttf\",v)\n            v = re.sub(\"lifestyle fit\",\"life_fit\",v)\n                    \n                # social \n            v = re.sub(\"social interactions on platforms\",\"soc_int_plt\", v)\n            v = re.sub(\"coercive pressures\", \"coe_prsr\",  v)\n            v = re.sub(\"coercive pressure\", \"coe_prsr\",  v)\n            v = re.sub(\"human human interaction\",\"hh_int\", v)\n            v = re.sub(\"human-human interaction\",\"hh_int\", v)\n            v = re.sub(\"social influence\", \"socinf\", v)\n            v = re.sub(\"collectivist cultural practices\", \"colcul\",  v)\n            v = re.sub(\"collectivist cultural practice\",\"colcul\", v)\n            v = re.sub(\"social media influence\",\"snsinf\", v)\n            v = re.sub(\"normative belief\",\"norm_blf\", v)\n            v = re.sub(\"interaction\",\"interac\", v)\n            v = re.sub(\"subjective norm\", \"sbj_nrm\",  v)\n            v = re.sub(\"subjective norms\", \"sbj_nrm\",  v)\n            v = re.sub(\"social factors\", \"soc_fac\",  v)\n            v = re.sub(\"social factor\" ,\"soc_fac\" , v)\n            v = re.sub(\"normative pressure\",\"nrm_prsr\", v)\n            v = re.sub(\"CSR economical responsibility\",\"csr_econ\", v)\n            v = re.sub(\"social norms\", \"soc_nrm\",  v)\n            v = re.sub(\"family influence\",\"fam_inf\", v)\n            v = re.sub(\"people\",\"people\", v)\n            v = re.sub(\"herd\",\"herd\", v)\n            v = re.sub(\"CSR social responsibility\",\"csr_soc\", v)\n            v = re.sub(\"mimetic pressure\", \"mim_prsr\",  v)\n            v = re.sub(\"CSR environmental responsibility\", \"csr_env\",  v)\n            v = re.sub(\"social value\",\"soc_val\", v)\n            v = re.sub(\"social values\",\"soc_val\", v)\n            v = re.sub(\"employee customer engagement\",\"engg_empcus\", v)\n            v = re.sub(\"employee-customer engagement\",\"engg_empcus\", v)\n            v = re.sub(\"social isolation\", \"soc_iso\",  v)\n            v = re.sub(\"normative pressures\",\"norm_prsr\", v)\n            v = re.sub(\"social proof social media\", \"soc_prf_sns\",  v)\n            v = re.sub(\"social proof\", \"soc_prf\",  v)\n            v = re.sub(\"social media\",\"sns\", v)\n            v = re.sub(\"word of mouth\", \"wom\",  v)\n            v = re.sub(\"wom\",\"wom\", v)\n            v = re.sub(\"word-of-mouth\", \"wom\",  v)\n            v = re.sub(\"w-o-m\", \"wom\",  v)\n            v = re.sub(\"wordmouth\", \"wom\",  v)\n            v = re.sub(\"environment\", \"env\",  v)\n                \n                \n                # psychological \n            v = re.sub(\"computer self efficacy\",\"self\",v)\n            v = re.sub(\"self efficacy\",\"self\",v)\n            v = re.sub(\"self-efficacy\", \"self\", v)\n            v = re.sub(\"attitude\", \"attd\", v)\n            v = re.sub(\"attitudes\",\"attd\",v)\n            v = re.sub(\"trust\",\"trst\",v)\n            v = re.sub(\"pragmatic\",\"prgt\",v)\n            v = re.sub(\"security concern\", \"sec_cn\",v)\n            v = re.sub(\"security concerns\",\"sec_cn\",v)\n            v = re.sub(\"self-image\", \"self_cong\", v)\n            v = re.sub(\"self image\",\"self_cong\",v)\n            v = re.sub(\"congruence\", \"cong\", v)\n            v = re.sub(\"self-congruence\",\"self_cong\",v)\n            v = re.sub(\"self congruence\", \"self_cong\", v)\n            v = re.sub(\"self-image congruence\",\"self_cong\",v)\n            v = re.sub(\"self image congruence\", \"self_cong\", v)\n            v = re.sub(\"selfimage congruence\", \"self_cong\", v)\n            v = re.sub(\"awareness\", \"awar\", v)\n            v = re.sub(\"satisfaction\",\"satis\",v)\n            v = re.sub(\"consumer satisfaction\",\"satis\",v)\n            v = re.sub(\"customer satisfaction\",\"satis\",v)\n            v = re.sub(\"restiant to change\",\"resist_chng\",v)\n            v = re.sub(\"resistance to change\",\"resist_chng\",v)\n            v = re.sub(\"risk aversion\", \"risk_avrs\", v)\n            v = re.sub(\"risk averse\",\"risk_avrs\",v)\n            v = re.sub(\"novelty\",\"new_seek\",v)\n            v = re.sub(\"novelty-seeking\", \"new_seek\", v)\n            v = re.sub(\"novelty seeking\", \"new_seek\", v)\n            v = re.sub(\"consciousnesnness\", \"conscn\", v)\n            v = re.sub(\"post-use trust\", \"post_trst\", v)\n            v = re.sub(\"post use trust\",\"post_trst\",v)\n            v = re.sub(\"postuse trust\",\"post_trst\",v)\n            v = re.sub(\"emotional experience\",\"emo_exp\",v)\n            v = re.sub(\"agreeableness\",\"agrbns\",v)\n            v = re.sub(\"privacy concerns\",\"priv_cn\",v)\n            v = re.sub(\"privacy concern\",\"priv_cn\",v)\n            v = re.sub(\"cognitive decline\",\"cog_dec\",v)\n            v = re.sub(\"benevolent convenince\",\"ben_conv\",v)\n            v = re.sub(\"enjoyment\",\"enjy\",v)\n            v = re.sub(\"enjoy\", \"enjy\", v)\n            v = re.sub(\"hedonic motivation\",\"hed_motiv\",v)\n            v = re.sub(\"oppenness\", \"open\", v)\n            v = re.sub(\"loyal\", \"loyal\", v)\n            v = re.sub(\"loyalty\",\"loyal\",v)\n            v = re.sub(\"confirmation\",\"confrm\",v)\n            v = re.sub(\"optimism\",\"optim\",v)\n            v = re.sub(\"safety concerns\",\"safe_cn\",v)\n            v = re.sub(\"safety concern\",\"safe_cn\",v)\n            v = re.sub(\"Covid-19 psychological distress\", \"dist_covid\", v)\n            v = re.sub(\"Covid19 psychological distress\", \"dist_covid\", v)\n            v = re.sub(\"Covid 19 psychological distress\",\"dist_covid\",v)\n            v = re.sub(\"psychological distress\",\"dist_covid\",v)\n            v = re.sub(\"green concerns\",\"green_cn\",v)\n            v = re.sub(\"technology anxiety\", \"anxiety\", v)\n            v = re.sub(\"anxiety\",\"anxiety\",v)\n            v = re.sub(\"obedience\", \"obed\", v)\n            v = re.sub(\"empathy\",\"empath\",v)\n            v = re.sub(\"decision comfort\",\"dec_comfrt\",v)\n            v = re.sub(\"confidence\",\"confdnc\",v)\n            v = re.sub(\"decision discomfort\", \"dec_dis_comfrt\", v)\n            v = re.sub(\"comfort\", \"cmfrt\", v)\n            v = re.sub(\"discomfort\",\"discmfrt\",v)\n            v = re.sub(\"insecurity\", \"insec\", v)\n            v = re.sub(\"insecurities\", \"insec\", v)\n            v = re.sub(\"benevolence\",\"benv\",v)\n            v = re.sub(\"technology stress\",\"tech_strss\",v)\n            v = re.sub(\"stress\", \"tech_strss\", v)\n            v = re.sub(\"techno-stress\",\"tech_strss\",v)\n            v = re.sub(\"technostress\",\"tech_strss\",v)\n            v = re.sub(\"techno stress\", \"tech_strss\", v)\n            v = re.sub(\"cognitive resistence\",\"cog_resist\",v)\n                    \n            # demographic \n            v = re.sub(\"age\",\"age\",v)\n            v = re.sub(\"sex\",\"sex\",v)\n            v = re.sub(\"education\",\"edu\",v)\n            v = re.sub(\"income\",\"income\",v)\n            v = re.sub(\"islamic religiosity\",\"religios\",v)\n            v = re.sub(\"culture\",\"cltr\",v)\n                \n            v = \" \".join([word for word in v.split() if word not in stop_words])\n            v = \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in v.split()])\n            #v = \" \".join([stemmer.stem(word) for word in v.split()])\n            dct[k] = v\n    return dct\n\ndef tokenizeToSentences(doc):\n    for k, v in doc.items():\n        \n        if isinstance(v, bytes):\n            v = v.decode('utf-8')\n          \n        v = v.lower()\n        v = v.replace('\\n', ' ')\n        v = re.sub(r'http\\S+www\\S+@\\S+', '', v)\n        #v = \" \".join([str(s) for s in v])\n\n        v = sent_tokenize(v)\n        doc[k] = v\n        \n    return doc\n\nFor Topic modeling, I write a function to generate dictionaries and save them in a .mm file format.\n\ndef generate_dictionary(text, name):\n    \"\"\" \n    As input takes in the text to build the dictionary for and the name of a .mm file\n    \"\"\" \n    \n    dictionary = Dictionary(text)\n    \n    corpus = [dictionary.doc2bow(review) for review in text] \n    \n    filename = f\"{name}.mm\"\n    \n    MmCorpus.serialize(filename, corpus)\n\n    return dictionary, corpus\n\nAdditionally, I want a function that prints the top 50 most frequently appearing words in the corpus:\n\n# ---------------------- START OF CHATGPT CODE\ndef print_top_50_words(corpus, dictionary):\n    total_word_count = defaultdict(int)\n    word_weights = defaultdict(float)\n\n    for word_id, word_count in itertools.chain.from_iterable(corpus):\n        total_word_count[word_id] += word_count\n\n    sorted_tota_words_count = sorted(total_word_count.items(), key = lambda w: w[1], reverse = True)\n\n    tfidf = TfidfModel(corpus)\n\n    for doc in corpus:\n        tfidf_weights = tfidf[doc]  # Calculate TF-IDF for the review\n        for term_id, weight in tfidf_weights:\n            word_weights[term_id] += weight  # Aggregate the weight for the term\n\n    sorted_word_weights = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n\n    # Print the top 50 terms with their weights\n    top_50_words = [(dictionary.get(term_id), weight) for term_id, weight in sorted_word_weights[:50]]\n\n    for word, weight in top_50_words:\n        print(word, weight)\n\n# ---------------------- END OF CHATGPT CODE \n\nI also plan on seeing how python clusters the words (as in, finds similar words) vs me:\n\ndef print_clusters(n_clusters, list_of_words):\n    clusters = {i: [] for i in range(n_clusters)}\n    for word, label in zip(list_of_words, labels):\n        clusters[label].append(word)\n\n    for label, words in clusters.items():\n        print(f\"Cluster {label}:\")\n        for word in words:\n            print(f\"  {word}\")\n        print(\"\\n\")\n\n    # Explain clusters\n    print(\"Cluster explanations based on semantics and ideas:\")\n    for label, words in clusters.items():\n        print(f\"Cluster {label} might be related to:\")\n        for word in words:\n            print(f\"  {word}\")\n        print(\"\\n\")\n\nThis is a function for if you want to use a word embedding (requires some effort, time and machine power!):\n\ndef get_embedding(text):\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    model_bert = BertModel.from_pretrained('bert-base-uncased')\n    \n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=20)\n    with torch.no_grad():\n        outputs = model_bert(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n\nAnd then you use this to get semantically similar words:\n\ndef get_semantically_similar_words(words, threshold=0.7):\n    similar_words = set(words)\n    for word in words:\n        token = nlp(word)\n        for vocab_word in nlp.vocab:\n            if vocab_word.has_vector and vocab_word.is_alpha:\n                similarity = token.similarity(nlp(vocab_word.text))\n                if similarity &gt;= threshold:\n                    similar_words.add(vocab_word.text)\n    return similar_words\n\nSo, how do I find the themes? Essentially, I just tweaked TF-IDF:\n\nclass CustomTfidfVectorizer(TfidfVectorizer):\n    def __init__(self, vocabulary=None, **kwargs):\n        super().__init__(vocabulary=vocabulary, **kwargs)\n        #self.general_keywords = set(general_keywords)\n        \n    def build_analyzer(self):\n        analyzer = super().build_analyzer()\n        return lambda doc: [w for w in analyzer(doc)] #if w not in self.general_keywords]\n    \n    def fit(self, raw_documents, y=None):\n        self.fit_transform(raw_documents, y)\n        return self\n\n    def fit_transform(self, raw_documents, y=None):\n        X = super().fit_transform(raw_documents, y)\n        self.max_frequencies = self._compute_max_frequencies(X, raw_documents)\n        return X\n\n    def transform(self, raw_documents):\n        X = super().transform(raw_documents)\n\n        # Calculate augmented term frequency\n        max_frequencies = self.max_frequencies\n        max_frequencies[max_frequencies == 0] = 1  # Avoid division by zero\n        augmented_tf = 0.5 + 0.5 * (X.toarray() / max_frequencies[:, None])\n        \n        # Penalize general keywords\n        #penalized_idf = self.idf_ * (1 - 0.8 * np.isin(self.get_feature_names_out(), list(self.general_keywords)))\n        \n        # Apply penalized IDF\n        augmented_tfidf = augmented_tf * penalized_idf\n\n        return csr_matrix(augmented_tfidf)\n\n    def _compute_max_frequencies(self, X, raw_documents):\n        max_frequencies = np.zeros(X.shape[0])\n        for i, doc in enumerate(raw_documents):\n            term_freq = {}\n            for term in doc.split():\n                if term in term_freq:\n                    term_freq[term] += 1\n                else:\n                    term_freq[term] = 1\n            max_frequencies[i] = max(term_freq.values())\n        return max_frequencies\n\n\n\n\n\ntry:\n    df = pd.read_csv(\"P2_AR_04.csv\", encoding='utf-8')\nexcept UnicodeDecodeError:\n    try:\n        df = pd.read_csv(\"P2_AR_04.csv\", encoding='latin-1')\n    except Exception as e:\n        error_message = str(e)\n        df = None\n\nClean all the data you‚Äôve gathered the same way the PDF‚Äôs have been cleaned (the preprocess_text() function looks very similar to the cleaning function above!):\n\ndf2 = df.copy()\n\ncolumns_to_preprocess = ['Man_Theme',\n                        'K1','K2','K3','K4','K5','K6','K7','K8','K9','K10',\n                        'F1','F2','F3','F4','F5','F6','F7','F8','F9',\n                        'FNS1','FNS2','FNS3','FNS4',\n                        'METHOD1','METHOD2','METHOD3','METHOD4',\n                        'THEORY1','THEORY2','THEORY3','THEORY4',\n                        'LIMIT1' ,'LIMIT2' ,'LIMIT3', 'Abstract'\n                        ]\n\nfor col in columns_to_preprocess:\n    df2[col] = df2[col].apply(preprocess_text)\n\n\npapers = {}\n\nfor paper_id, filename in name_of_pdfs.items():\n    text = extract_text_from_pdf(filename)\n    papers[paper_id] = text\n\npapers_df = pd.DataFrame.from_dict(papers, orient = 'index', columns = ['paperText'])\npapers_df = papers_df.reset_index(names = ['paperID'])\npapers_df.to_csv('papers_unclean.csv')\npapers_df.head()\n\nThese look like this:",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR"
    ]
  },
  {
    "objectID": "study1.html#part-1.-data-collection",
    "href": "study1.html#part-1.-data-collection",
    "title": "The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "section": "",
    "text": "I downloaded the pdf of all the papers (143), reading them and extracting meta data based on the following:\n\nimport numpy as np \n\ndatabase = np.array([\n    {\n        'id': 'string', # unique identifier for the paper following convention P2_#number \n        'title': 'string', # title of the paper\n        'AffiliationCountry': 'string' , #name of country the study was conducted in,\n        'year': 2018-2024, # year of publication a value between 2018 and 2024\n        'journal': 'string', # name of the journal the paper was published in\n        'citations': 0-1000, # number of citations the paper has received - not reported in the paper \n        'year_since': 3, # number of years since publication - not reported in the paper \n        'cpy': 0, # number of citations per year - not reported in the paper \n        'keywords': ['TAM', 'mbanking', 'awareness'], # list of keywords, broken into K1-K10\n        'abstract': 'string', # abstract of the paper \n        'F': ['perceived usefulness'], # factors significant in the study, broken into F1-F9 \n        'FN': ['another factor'], # factors not significant in the study, broken into FNS1-FNS4 \n        'limit': ['geographical context'], # limitations of the study, broken into LIMIT1-LIMIT3 \n        'typeofResearch': 'string', # type of research conducted in the study \n        'methods': ['regression analysis'], # methods used in the study, broken into METHOD1-METHOD4\n        'theory': ['TAM'] # theories used in the study, broken into THEORY1-THEORY4\n        'sampleSize': 100, # sample size of the study \n        'tech': 'string', # main technology studied \n        'man_theme': 'string', # Theme manually assigned by me \n        'algo_theme': 'string', # Theme assigned by the algorithm \n        'decision_Theme': 'string', # Final theme of the paper  \n        'Score_Sig': 0.0, # % of significance for factors \n        'Score_NOT_Sig': 0.0, # % of non-significance for factors\n    }\n])\n\n\n\nIdea for future\n\nü§ñ Build an Agentic AI application that automates this process.\n\n\n\nFirst, install the following Python modules:\n\n%%capture \n!pip install nltk\n!pip install gensim\n!pip install itertools\n!pip install spacy\n!pip install langdetect\n!pip install pprint\n!pip install pyLDAvis\n!pip install textract\n!pip install spacy\n!pip install pymupdf\n!pip uninstall matplotlib seaborn -y\n!pip install matplotlib seaborn  \n!pip install --upgrade matplotlib seaborn\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n!pip install scipy==1.12.0 --quiet\n\nSince I am not familiar with Docker, I couldn‚Äôt resolve the package dependencies. This took so much time for me and I finally managed to fix it with this specific configuration. The imports look scary:\n\nimport string\nimport os \nimport re # regular expression \nimport pandas as pd\nimport numpy as np\n__requires__= 'scipy==1.12.0'\nimport scipy \nimport itertools\nimport textract # PDF text extraction \nimport math\nimport spacy\nimport fitz #PyMuPDF - another (better) PDF text extraction \n\n#NLP imports\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.util import ngrams\nfrom nltk.tokenize import RegexpTokenizer\n# from nltk import pos_tag # didn't actually use it \n\n#SKLEARN\nfrom sklearn import metrics\nfrom sklearn import neighbors\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\n# from sklearn.decomposition import PCA\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.metrics import classification_report\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.neighbors import NearestNeighbors\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n# from sklearn.naive_bayes import (\n# BernoulliNB,\n# ComplementNB,\n# MultinomialNB,\n# )\n#from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.svm import SVC\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.neural_network import MLPClassifier\n# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics.pairwise import cosine_similarity\n#from sklearn.decomposition import LatentDirichletAllocation\n\n#GENSIMimports\nimport gensim\nfrom gensim.models import Phrases\nfrom gensim.models.phrases import Phraser\nfrom gensim.corpora.dictionary import Dictionary\nfrom gensim.corpora import MmCorpus\nfrom gensim.models.tfidfmodel import TfidfModel\nfrom gensim.models import CoherenceModel\nfrom gensim.models import KeyedVectors\n\n#PyLDAvis imports for visualization of topic modeling results \n# import pyLDAvis\n# import pyLDAvis.gensim_models as gensimvis\n# import pyLDAvis.gensim\n# import pyLDAvis.gensim_models\n\n#MISC imports\nfrom collections import Counter\nfrom collections import defaultdict\nfrom string import punctuation\nfrom pprint import pprint\nfrom numpy import triu\n#from scipy.linalg.special_matrices import triu\nfrom scipy.sparse import csr_matrix\n\n#TRANSFORMERS\n#import torch\n#importtensorflowastf\n#from transformers import BertTokenizer, BertModel\n#from transformers import AutoTokenizer, AutoModel\n#fromtensorflow.keras.modelsimportSequential\n#fromtensorflow.keras.preprocessing.textimportTokenizer\n#fromtensorflow.keras.preprocessing.sequenceimportpad_sequences\n#fromtensorflow.keras.layersimportDense,Embedding,LSTM,SpatialDropout1D\n#fromtensorflow.keras.layersimportLeakyReLU\n\n#MATPLOT\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nDownload some of the language support stuff:\n\n# only run once\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('punkt_tab')\nnltk.download('omw-1.4')  # Optional \n#nltk.download('averaged_perceptron_tagger')  # For POS tagging\n#nltk.download('averaged_perceptron_tagger_eng') # POS tagging \n\nI saved the pdf files‚Äô name in a dictionary like this:\n\nname_of_pdfs = {\n    'p2_01': \"Lonkani et al_2020_A comparative study of trust in mobile banking.pdf\", \n    'p2_02': \"Saprikis et al_2022_A comparative study of users versus non-users' behavioral intention towards.pdf\", \n    'p2_03': \"Malaquias et al_2021_A cross-country study on intention to use mobile banking.pdf\", \n    'p2_04': \"Merhi et al_2019_A cross-cultural study of the intention to use mobile banking between Lebanese.pdf\", \n    'p2_05': \"Frimpong et al. - 2020 - A cross‚Äênational investigation of trait antecedent.pdf\", \n    # and so on ... \n}\n\nAdditionally, I defined a dictionary ‚Äúlook up‚Äù for all the factors in the dataset with their related theme that looks like this (shortened for this presentation):\n\ntheme_of_words = {\n    'demographic': \n        list(set(['women', 'woman', 'female', 'men', 'man', 'male', 'sex', 'gender', 'age', 'income', \n            'demographic variables', 'elderly', 'education', 'gender differences', 'generation y', 'millennial generation',\n            'millennial', 'gen y', 'gen Z', 'gen alpha', 'gen X', 'boomer', 'babyboomer', 'generation X', 'generation z',\n            'young consumers', \n            # A lot more factors ...\n            ])),\n    \n    #----------------------------------------------------------------------------------------------------------------------------------\n    'cultural': \n        list(set(['developing countries','malaysia','transition country','pakistan',\n            'zakat','developing country','ghana','USA','srilanka', 'sri lanka',\n            'india','maldives','saudi-arabia','saudi arabia', 'nigeria','thailand','united states',\n            'yemen','citizenship','zimbabwe','palestine','culture',\n            'Country perspective', \n            # ... \n            ])),\n    \n    #----------------------------------------------------------------------------------------------------------------------------------\n    'psychological':\n        list(set(['anxiety','satisfaction','behavior','behaviour','attitudes','attitude','awareness',\n            'technology anxiety','consumer-behavior','trust','benv','consumer behaviour',\n            'covid-19 related psychological distress','psychological distress','psychological','distress',\n            'behavioral','computer anxiety','customer satisfaction', 'cognitive resistance',\n            # A LOT more ... \n            ]))\n            , \n            # ... few other key value pairs corresponding to themes \n\n}\n\nI also needed to delete some stop words, and decided to add more words that I knew would be frequently repeated. I also define the lemmer and stemmer.\n\nstop_words = stopwords.words('english')\nstop_words.extend([\"bank\", \"banking\", \"banks\", \n                   \"mobile\", \"mbank\", \"mbanking\", \"m-bank\", \"m bank\",\n                   \"online\", \"e\", \"e-bank\", \"ebank\", \"mobile banking\", \"mobile bank\", \n                   \"adoption\", \"acceptance\", \"accept\", \"theory\", \n                   \"purpose\", \"result\", \"method\", #from abstracts \n                   \"journal\", \"volume\", \"pp\", \"no\", \"doi\", \"http\", \"https\", \"et al\", \"issue\",\n                   \"technology\", \"internet\", \"information system\", \"international information\",\n                   \"information technology\", \"computer human\", \"mis quarterly\", \"electornic commerce\",\n                   \"j market\", \"telematics and informatics\", \"telematics informatics\", \"retail consumer\",\n                   \"international volume\", \"international business\", \"global information\",\n                   \"et\", \"al\", \"al.\", \"tam\", \"sem\", \"pls\", \"utaut\", \"tpb\",\n                   \".com\", \"management\", \"marketing\", \"published\", \"study\",\n                   \"research\", \"literature\", \"model\", #from journal information \n                   \"app\", \"application\", \"usage\"])\n\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\n\n\nSo, I need a few functions as set up for cleaning the text. Function extract_text_from_pdf() is using PyMuPDF to extract text from a PDF file.\n\n#version one using PyMuPDF \ndef extract_text_from_pdf(filename):\n    text = \"\"\n    try:\n        doc = fitz.open(filename)\n        for page_num in range(doc.page_count):\n            page = doc.load_page(page_num)\n            text += page.get_text()\n    except Exception as e:\n        print(f\"Error reading {filename}: {e}\")\n    return text\n\nThis function is just one of the data cleaning functions:\n\ndef preprocess_Dict(dct):\n    for k, v in dct.items():\n        if isinstance(v, list):\n            processed_list = []\n            for item in v:\n                item = item.lower()\n                item = re.sub(r'http\\S+|www\\S+|@\\S+', '', item)\n                item = re.sub(r'(?&lt;=\\w)-(?=\\w)', ' ', item)\n                item = re.sub(r'[^a-z0-9\\s\\n]', '', item)\n                item = re.sub(r'\\s+', ' ', item).strip()\n                item = re.sub(r'\\d+', '', item).strip()\n\n                # replacing abbreviations \n                item = item.replace('structural equation model', 'sem')\n                item = item.replace('technology acceptance model', 'tam')\n                item = item.replace('unified theory of acceptance and use of technology', 'utaut')\n                item = item.replace('diffusion of innovation', 'doi')\n                item = item.replace('partial least squares', 'pls')\n                item = item.replace('theory of planned behavior', 'tpb')\n                item = re.sub(\"perceived usefulness\", \"pu\", item)\n                item = re.sub(\"perceived ease of use\", \"peou\", item)\n                item = re.sub(\"perceived privacy\", \"priv\", item)\n                item = re.sub(\"perceived aesthetics\", \"p_aest\", item)\n                item = re.sub(\"perceived relative advantage\", \"p_rel_adv\", item)\n                item = re.sub(\"perceived risk\", \"prisk\", item)\n                item = re.sub(\"perceived enjoyment\",\"penjy\", item)\n                item = re.sub(\"perceived intelligence\",\"pintlj\", item)\n                item = re.sub(\"perceived security\",\"psec\", item)\n                item = re.sub(\"perceived trust\",\"ptrst\", item)\n                item = re.sub(\"perceived anthropomorphism\",\"panthro\", item)\n                item = re.sub(\"perceived value\",\"pval\", item)\n                item = re.sub(\"perceived compatibility\",\"pcompat\", item)\n                item = re.sub(\"perceived detterants\",\"pdet\", item)\n                item = re.sub(\"perceived behavioral control\", \"p_bhv_ctrl\", item)\n                item = re.sub(\"perceived credibility\",\"pcred\", item)\n                item = re.sub(\"perceived cost\",\"pcost\", item)\n                item = re.sub(\"perceived benefit\",\"pbenef\", item)\n                item = re.sub(\"perceived convenience\",\"pconv\", item)\n                item = re.sub(\"perceived usability\",\"pusbl\", item)\n                item = re.sub(\"perceived privacy concerns\", \"ppriv_cn\", item)\n                \n                # belief base \n                item = re.sub(\"performance expectancy\", \"peex\", item)\n                item = re.sub(\"convenience\", \"conv\", item)\n                item = re.sub(\"effort expectancy\",\"efex\", item)\n                item = re.sub(\"access convenience\",\"acc_conv\", item)\n                item = re.sub(\"reliability\", \"rely\", item)\n                item = re.sub(\"behavioral control\", \"bhv_ctrl\", item)\n                item = re.sub(\"compatibility\", \"compat\", item)\n                item = re.sub(\"normative beliefs\", \"norm_blf\", item)\n                item = re.sub(\"normative belief\", \"norm_blf\", item)\n                item = re.sub(\"transaction convenience\",\"trans_conv\", item)\n                item = re.sub(\"post use trust\", \"post_trst\", item)\n                item = re.sub(\"post-use trust\", \"post_trst\", item)\n                item = re.sub(\"benefit convenience\",\"ben_conv\", item)\n                item = re.sub(\"search convenience\", \"srch_conv\", item)\n                item = re.sub(\"utilitarian expectation\", \"util_exp\", item)\n                item = re.sub(\"evaluation convenience\", \"eval_conv\", item)\n                item = re.sub(\"expectation\", \"expect\", item)\n                item = re.sub(\"possession convenience\", \"poss_conv\", item)\n                item = re.sub(\"expected advantage\", \"exp_adv\", item)\n               \n                # intention \n                item = re.sub(\"intention\", \"intnt\", item)\n                item = re.sub(\"motivation\", \"motiv\", item)\n                item = re.sub(\"automative motivation\",\"auto_motiv\", item)\n                item = re.sub(\"behavioral intention\",\"bhv_intnt\", item)\n                item = re.sub(\"control motivation\",\"ctrl_motiv\", item)\n                item = re.sub(\"controlled motivation\", \"ctrl_motiv\",  item)\n                item = re.sub(\"hedonic motivation\",\"hed_motiv\", item)\n                item = re.sub(\"intention to use\",\"intnt_use\", item)\n                \n                # personal \n                item = re.sub(\"habit\", \"habt\", item)\n                item = re.sub(\"personality\",\"prsnl\",item)\n                item = re.sub(\"personal factors\",\"prsnl\",item)\n                item = re.sub(\"personal factor\", \"prsnl\", item)\n                item = re.sub(\"digital literacy\",\"dig_lit\",item)\n                item = re.sub(\"digital capability\",\"dig_cabl\",item)\n                item = re.sub(\"agreeableness\",\"agrbns\",item)\n                item = re.sub(\"financial literacy\",\"fin_lit\",item)\n                item = re.sub(\"previous experience\",\"prv_exp\",item)\n                item = re.sub(\"life compatibility\",\"life_compat\",item)\n                item = re.sub(\"lifestyle\", \"life\", item)\n                item = re.sub(\"knowledge\",\"know\",item)\n                item = re.sub(\"functional value\",\"fun_val\",item)\n                item = re.sub(\"fun value\", \"fun_val\", item)\n                item = re.sub(\"utalitarian value\",\"util_val\",item)\n                item = re.sub(\"epistemic value\",\"epi_val\",item)\n                item = re.sub(\"monetary value\",\"mon_val\",item)\n                item = re.sub(\"money value\",\"mon_val\",item)\n                item = re.sub(\"hedonic value\", \"hed_val\", item)\n                item = re.sub(\"emotional value\",\"emo_val\",item)\n                item = re.sub(\"quality value\",\"qual_val\",item)\n                item = re.sub(\"value barriers\", \"val_bar\", item)\n                item = re.sub(\"value barrier\",\"val_bar\",item)\n                item = re.sub(\"customer experience about usability\",\"exp_use\",item)\n                item = re.sub(\"experience\",\"exp\",item)\n                item = re.sub(\"self employment\",\"semp\",item)\n                item = re.sub(\"self-employment\",\"semp\",item)\n                item = re.sub(\"valence\",\"val\",item)\n                item = re.sub(\"religiosity\",\"religis\",item)\n                item = re.sub(\"task technology fit\",\"ttf\",item)\n                item = re.sub(\"lifestyle fit\",\"life_fit\",item)\n                    \n                # social \n                item = re.sub(\"social interactions on platforms\",\"soc_int_plt\", item)\n                item = re.sub(\"coercive pressures\", \"coe_prsr\",  item)\n                item = re.sub(\"coercive pressure\", \"coe_prsr\",  item)\n                item = re.sub(\"human human interaction\",\"hh_int\", item)\n                item = re.sub(\"human-human interaction\",\"hh_int\", item)\n                item = re.sub(\"social influence\", \"socinf\", item)\n                item = re.sub(\"collectivist cultural practices\", \"colcul\",  item)\n                item = re.sub(\"collectivist cultural practice\",\"colcul\", item)\n                item = re.sub(\"social media influence\",\"snsinf\", item)\n                item = re.sub(\"normative belief\",\"norm_blf\", item)\n                item = re.sub(\"interaction\",\"interac\", item)\n                item = re.sub(\"subjective norm\", \"sbj_nrm\",  item)\n                item = re.sub(\"subjective norms\", \"sbj_nrm\",  item)\n                item = re.sub(\"social factors\", \"soc_fac\",  item)\n                item = re.sub(\"social factor\" ,\"soc_fac\" , item)\n                item = re.sub(\"normative pressure\",\"nrm_prsr\", item)\n                item = re.sub(\"CSR economical responsibility\",\"csr_econ\", item)\n                item = re.sub(\"social norms\", \"soc_nrm\",  item)\n                item = re.sub(\"family influence\",\"fam_inf\", item)\n                item = re.sub(\"people\",\"people\", item)\n                item = re.sub(\"herd\",\"herd\", item)\n                item = re.sub(\"CSR social responsibility\",\"csr_soc\", item)\n                item = re.sub(\"mimetic pressure\", \"mim_prsr\",  item)\n                item = re.sub(\"CSR environmental responsibility\", \"csr_env\",  item)\n                item = re.sub(\"social value\",\"soc_val\", item)\n                item = re.sub(\"social values\",\"soc_val\", item)\n                item = re.sub(\"employee customer engagement\",\"engg_empcus\", item)\n                item = re.sub(\"employee-customer engagement\",\"engg_empcus\", item)\n                item = re.sub(\"social isolation\", \"soc_iso\",  item)\n                item = re.sub(\"normative pressures\",\"norm_prsr\", item)\n                item = re.sub(\"social proof social media\", \"soc_prf_sns\",  item)\n                item = re.sub(\"social proof\", \"soc_prf\",  item)\n                item = re.sub(\"social media\",\"sns\", item)\n                item = re.sub(\"word of mouth\", \"wom\",  item)\n                item = re.sub(\"wom\",\"wom\", item)\n                item = re.sub(\"word-of-mouth\", \"wom\",  item)\n                item = re.sub(\"w-o-m\", \"wom\",  item)\n                item = re.sub(\"wordmouth\", \"wom\",  item)\n                item = re.sub(\"environment\", \"env\",  item)\n                \n                \n                # psychological \n                item = re.sub(\"computer self efficacy\",\"self\",item)\n                item = re.sub(\"self efficacy\",\"self\",item)\n                item = re.sub(\"self-efficacy\", \"self\", item)\n                item = re.sub(\"attitude\", \"attd\", item)\n                item = re.sub(\"attitudes\",\"attd\",item)\n                item = re.sub(\"trust\",\"trst\",item)\n                item = re.sub(\"pragmatic\",\"prgt\",item)\n                item = re.sub(\"security concern\", \"sec_cn\",item)\n                item = re.sub(\"security concerns\",\"sec_cn\",item)\n                item = re.sub(\"self-image\", \"self_cong\", item)\n                item = re.sub(\"self image\",\"self_cong\",item)\n                item = re.sub(\"congruence\", \"cong\", item)\n                item = re.sub(\"self-congruence\",\"self_cong\",item)\n                item = re.sub(\"self congruence\", \"self_cong\", item)\n                item = re.sub(\"self-image congruence\",\"self_cong\",item)\n                item = re.sub(\"self image congruence\", \"self_cong\", item)\n                item = re.sub(\"selfimage congruence\", \"self_cong\", item)\n                item = re.sub(\"awareness\", \"awar\", item)\n                item = re.sub(\"satisfaction\",\"satis\",item)\n                item = re.sub(\"consumer satisfaction\",\"satis\",item)\n                item = re.sub(\"customer satisfaction\",\"satis\",item)\n                item = re.sub(\"restiant to change\",\"resist_chng\",item)\n                item = re.sub(\"resistance to change\",\"resist_chng\",item)\n                item = re.sub(\"risk aversion\", \"risk_avrs\", item)\n                item = re.sub(\"risk averse\",\"risk_avrs\",item)\n                item = re.sub(\"novelty\",\"new_seek\",item)\n                item = re.sub(\"novelty-seeking\", \"new_seek\", item)\n                item = re.sub(\"novelty seeking\", \"new_seek\", item)\n                item = re.sub(\"consciousnesnness\", \"conscn\", item)\n                item = re.sub(\"post-use trust\", \"post_trst\", item)\n                item = re.sub(\"post use trust\",\"post_trst\",item)\n                item = re.sub(\"postuse trust\",\"post_trst\",item)\n                item = re.sub(\"emotional experience\",\"emo_exp\",item)\n                item = re.sub(\"agreeableness\",\"agrbns\",item)\n                item = re.sub(\"privacy concerns\",\"priv_cn\",item)\n                item = re.sub(\"privacy concern\",\"priv_cn\",item)\n                item = re.sub(\"cognitive decline\",\"cog_dec\",item)\n                item = re.sub(\"benevolent convenince\",\"ben_conv\",item)\n                item = re.sub(\"enjoyment\",\"enjy\",item)\n                item = re.sub(\"enjoy\", \"enjy\", item)\n                item = re.sub(\"hedonic motivation\",\"hed_motiv\",item)\n                item = re.sub(\"oppenness\", \"open\", item)\n                item = re.sub(\"loyal\", \"loyal\", item)\n                item = re.sub(\"loyalty\",\"loyal\",item)\n                item = re.sub(\"confirmation\",\"confrm\",item)\n                item = re.sub(\"optimism\",\"optim\",item)\n                item = re.sub(\"safety concerns\",\"safe_cn\",item)\n                item = re.sub(\"safety concern\",\"safe_cn\",item)\n                item = re.sub(\"Covid-19 psychological distress\", \"dist_covid\", item)\n                item = re.sub(\"Covid19 psychological distress\", \"dist_covid\", item)\n                item = re.sub(\"Covid 19 psychological distress\",\"dist_covid\",item)\n                item = re.sub(\"psychological distress\",\"dist_covid\",item)\n                item = re.sub(\"green concerns\",\"green_cn\",item)\n                item = re.sub(\"technology anxiety\", \"anxiety\", item)\n                item = re.sub(\"anxiety\",\"anxiety\",item)\n                item = re.sub(\"obedience\", \"obed\", item)\n                item = re.sub(\"empathy\",\"empath\",item)\n                item = re.sub(\"decision comfort\",\"dec_comfrt\",item)\n                item = re.sub(\"confidence\",\"confdnc\",item)\n                item = re.sub(\"decision discomfort\", \"dec_dis_comfrt\", item)\n                item = re.sub(\"comfort\", \"cmfrt\", item)\n                item = re.sub(\"discomfort\",\"discmfrt\",item)\n                item = re.sub(\"insecurity\", \"insec\", item)\n                item = re.sub(\"insecurities\", \"insec\", item)\n                item = re.sub(\"benevolence\",\"benv\",item)\n                item = re.sub(\"technology stress\",\"tech_strss\",item)\n                item = re.sub(\"stress\", \"tech_strss\", item)\n                item = re.sub(\"techno-stress\",\"tech_strss\",item)\n                item = re.sub(\"technostress\",\"tech_strss\",item)\n                item = re.sub(\"techno stress\", \"tech_strss\", item)\n                item = re.sub(\"cognitive resistence\",\"cog_resist\",item)\n                        \n                # demographic \n                item = re.sub(\"age\",\"age\",item)\n                item = re.sub(\"sex\",\"sex\",item)\n                item = re.sub(\"education\",\"edu\",item)\n                item = re.sub(\"income\",\"income\",item)\n                item = re.sub(\"islamic religiosity\",\"religios\",item)\n                item = re.sub(\"culture\",\"cltr\",item)\n                \n                item = \" \".join([word for word in item.split() if word not in stop_words])\n                item = \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in item.split()])\n                #item = \" \".join([stemmer.stem(word) for word in item.split()])\n                processed_list.append(item)\n            dct[k] = \" \".join(processed_list)\n        else:\n            v = v.lower()\n            v = re.sub(r'http\\S+|www\\S+|@\\S+', '', v)\n            v = re.sub(r'(?&lt;=\\w)-(?=\\w)', ' ', v)\n            v = re.sub(r'[^a-z0-9\\s\\n]', '', v)\n            v = re.sub(r'\\s+', ' ', v).strip()\n            v = re.sub(r'\\d+', '', v).strip()\n\n            # replacing abbreviations \n            v = v.replace('structural equation model', 'sem')\n            v = v.replace('technology acceptance model', 'tam')\n            v = v.replace('unified theory of acceptance and use of technology', 'utaut')\n            v = v.replace('diffusion of innovation', 'doi')\n            v = v.replace('partial least squares', 'pls')\n            v = v.replace('theory of planned behavior', 'tpb')\n            v = re.sub(\"perceived usefulness\", \"pu\", v)\n            v = re.sub(\"perceived ease of use\", \"peou\", v)\n            v = re.sub(\"perceived privacy\", \"priv\", v)\n            v = re.sub(\"perceived aesthetics\", \"p_aest\", v)\n            v = re.sub(\"perceived relative advantage\", \"p_rel_adv\", v)\n            v = re.sub(\"perceived risk\", \"prisk\", v)\n            v = re.sub(\"perceived enjoyment\",\"penjy\", v)\n            v = re.sub(\"perceived intelligence\",\"pintlj\", v)\n            v = re.sub(\"perceived security\",\"psec\", v)\n            v = re.sub(\"perceived trust\",\"ptrst\", v)\n            v = re.sub(\"perceived anthropomorphism\",\"panthro\", v)\n            v = re.sub(\"perceived value\",\"pval\", v)\n            v = re.sub(\"perceived compatibility\",\"pcompat\", v)\n            v = re.sub(\"perceived detterants\",\"pdet\", v)\n            v = re.sub(\"perceived behavioral control\", \"p_bhv_ctrl\", v)\n            v = re.sub(\"perceived credibility\",\"pcred\", v)\n            v = re.sub(\"perceived cost\",\"pcost\", v)\n            v = re.sub(\"perceived benefit\",\"pbenef\", v)\n            v = re.sub(\"perceived convenience\",\"pconv\", v)\n            v = re.sub(\"perceived usability\",\"pusbl\", v)\n            v = re.sub(\"perceived privacy concerns\", \"ppriv_cn\", v)\n                \n            v = re.sub(\"performance expectancy\", \"peex\", v)\n            v = re.sub(\"convenience\", \"conv\", v)\n            v = re.sub(\"effort expectancy\",\"efex\", v)\n            v = re.sub(\"access convenience\",\"acc_conv\", v)\n            v = re.sub(\"reliability\", \"rely\", v)\n            v = re.sub(\"behavioral control\", \"bhv_ctrl\", v)\n            v = re.sub(\"compatibility\", \"compat\", v)\n            v = re.sub(\"normative beliefs\", \"norm_blf\", v)\n            v = re.sub(\"normative belief\", \"norm_blf\", v)\n            v = re.sub(\"transaction convenience\",\"trans_conv\", v)\n            v = re.sub(\"post use trust\", \"post_trst\", v)\n            v = re.sub(\"post-use trust\", \"post_trst\", v)\n            v = re.sub(\"benefit convenience\",\"ben_conv\", v)\n            v = re.sub(\"search convenience\", \"srch_conv\", v)\n            v = re.sub(\"utilitarian expectation\", \"util_exp\", v)\n            v = re.sub(\"evaluation convenience\", \"eval_conv\", v)\n            v = re.sub(\"expectation\", \"expect\", v)\n            v = re.sub(\"possession convenience\", \"poss_conv\", v)\n            v = re.sub(\"expected advantage\", \"exp_adv\", v)\n            \n            # intention \n            v = re.sub(\"intention\", \"intnt\", v)\n            v = re.sub(\"motivation\", \"motiv\", v)\n            v = re.sub(\"automative motivation\",\"auto_motiv\", v)\n            v = re.sub(\"behavioral intention\",\"bhv_intnt\", v)\n            v = re.sub(\"control motivation\",\"ctrl_motiv\", v)\n            v = re.sub(\"controlled motivation\", \"ctrl_motiv\",  v)\n            v = re.sub(\"hedonic motivation\",\"hed_motiv\", v)\n            v = re.sub(\"intention to use\",\"intnt_use\", v)\n                \n                # personal \n            v = re.sub(\"habit\", \"habt\", v)\n            v = re.sub(\"personality\",\"prsnl\",v)\n            v = re.sub(\"personal factors\",\"prsnl\",v)\n            v = re.sub(\"personal factor\", \"prsnl\", v)\n            v = re.sub(\"digital literacy\",\"dig_lit\",v)\n            v = re.sub(\"digital capability\",\"dig_cabl\",v)\n            v = re.sub(\"agreeableness\",\"agrbns\",v)\n            v = re.sub(\"financial literacy\",\"fin_lit\",v)\n            v = re.sub(\"previous experience\",\"prv_exp\",v)\n            v = re.sub(\"life compatibility\",\"life_compat\",v)\n            v = re.sub(\"lifestyle\", \"life\", v)\n            v = re.sub(\"knowledge\",\"know\",v)\n            v = re.sub(\"functional value\",\"fun_val\",v)\n            v = re.sub(\"fun value\", \"fun_val\", v)\n            v = re.sub(\"utalitarian value\",\"util_val\",v)\n            v = re.sub(\"epistemic value\",\"epi_val\",v)\n            v = re.sub(\"monetary value\",\"mon_val\",v)\n            v = re.sub(\"money value\",\"mon_val\",v)\n            v = re.sub(\"hedonic value\", \"hed_val\", v)\n            v = re.sub(\"emotional value\",\"emo_val\",v)\n            v = re.sub(\"quality value\",\"qual_val\",v)\n            v = re.sub(\"value barriers\", \"val_bar\", v)\n            v = re.sub(\"value barrier\",\"val_bar\",v)\n            v = re.sub(\"customer experience about usability\",\"exp_use\",v)\n            v = re.sub(\"experience\",\"exp\",v)\n            v = re.sub(\"self employment\",\"semp\",v)\n            v = re.sub(\"self-employment\",\"semp\",v)\n            v = re.sub(\"valence\",\"val\",v)\n            v = re.sub(\"religiosity\",\"religis\",v)\n            v = re.sub(\"task technology fit\",\"ttf\",v)\n            v = re.sub(\"lifestyle fit\",\"life_fit\",v)\n                    \n                # social \n            v = re.sub(\"social interactions on platforms\",\"soc_int_plt\", v)\n            v = re.sub(\"coercive pressures\", \"coe_prsr\",  v)\n            v = re.sub(\"coercive pressure\", \"coe_prsr\",  v)\n            v = re.sub(\"human human interaction\",\"hh_int\", v)\n            v = re.sub(\"human-human interaction\",\"hh_int\", v)\n            v = re.sub(\"social influence\", \"socinf\", v)\n            v = re.sub(\"collectivist cultural practices\", \"colcul\",  v)\n            v = re.sub(\"collectivist cultural practice\",\"colcul\", v)\n            v = re.sub(\"social media influence\",\"snsinf\", v)\n            v = re.sub(\"normative belief\",\"norm_blf\", v)\n            v = re.sub(\"interaction\",\"interac\", v)\n            v = re.sub(\"subjective norm\", \"sbj_nrm\",  v)\n            v = re.sub(\"subjective norms\", \"sbj_nrm\",  v)\n            v = re.sub(\"social factors\", \"soc_fac\",  v)\n            v = re.sub(\"social factor\" ,\"soc_fac\" , v)\n            v = re.sub(\"normative pressure\",\"nrm_prsr\", v)\n            v = re.sub(\"CSR economical responsibility\",\"csr_econ\", v)\n            v = re.sub(\"social norms\", \"soc_nrm\",  v)\n            v = re.sub(\"family influence\",\"fam_inf\", v)\n            v = re.sub(\"people\",\"people\", v)\n            v = re.sub(\"herd\",\"herd\", v)\n            v = re.sub(\"CSR social responsibility\",\"csr_soc\", v)\n            v = re.sub(\"mimetic pressure\", \"mim_prsr\",  v)\n            v = re.sub(\"CSR environmental responsibility\", \"csr_env\",  v)\n            v = re.sub(\"social value\",\"soc_val\", v)\n            v = re.sub(\"social values\",\"soc_val\", v)\n            v = re.sub(\"employee customer engagement\",\"engg_empcus\", v)\n            v = re.sub(\"employee-customer engagement\",\"engg_empcus\", v)\n            v = re.sub(\"social isolation\", \"soc_iso\",  v)\n            v = re.sub(\"normative pressures\",\"norm_prsr\", v)\n            v = re.sub(\"social proof social media\", \"soc_prf_sns\",  v)\n            v = re.sub(\"social proof\", \"soc_prf\",  v)\n            v = re.sub(\"social media\",\"sns\", v)\n            v = re.sub(\"word of mouth\", \"wom\",  v)\n            v = re.sub(\"wom\",\"wom\", v)\n            v = re.sub(\"word-of-mouth\", \"wom\",  v)\n            v = re.sub(\"w-o-m\", \"wom\",  v)\n            v = re.sub(\"wordmouth\", \"wom\",  v)\n            v = re.sub(\"environment\", \"env\",  v)\n                \n                \n                # psychological \n            v = re.sub(\"computer self efficacy\",\"self\",v)\n            v = re.sub(\"self efficacy\",\"self\",v)\n            v = re.sub(\"self-efficacy\", \"self\", v)\n            v = re.sub(\"attitude\", \"attd\", v)\n            v = re.sub(\"attitudes\",\"attd\",v)\n            v = re.sub(\"trust\",\"trst\",v)\n            v = re.sub(\"pragmatic\",\"prgt\",v)\n            v = re.sub(\"security concern\", \"sec_cn\",v)\n            v = re.sub(\"security concerns\",\"sec_cn\",v)\n            v = re.sub(\"self-image\", \"self_cong\", v)\n            v = re.sub(\"self image\",\"self_cong\",v)\n            v = re.sub(\"congruence\", \"cong\", v)\n            v = re.sub(\"self-congruence\",\"self_cong\",v)\n            v = re.sub(\"self congruence\", \"self_cong\", v)\n            v = re.sub(\"self-image congruence\",\"self_cong\",v)\n            v = re.sub(\"self image congruence\", \"self_cong\", v)\n            v = re.sub(\"selfimage congruence\", \"self_cong\", v)\n            v = re.sub(\"awareness\", \"awar\", v)\n            v = re.sub(\"satisfaction\",\"satis\",v)\n            v = re.sub(\"consumer satisfaction\",\"satis\",v)\n            v = re.sub(\"customer satisfaction\",\"satis\",v)\n            v = re.sub(\"restiant to change\",\"resist_chng\",v)\n            v = re.sub(\"resistance to change\",\"resist_chng\",v)\n            v = re.sub(\"risk aversion\", \"risk_avrs\", v)\n            v = re.sub(\"risk averse\",\"risk_avrs\",v)\n            v = re.sub(\"novelty\",\"new_seek\",v)\n            v = re.sub(\"novelty-seeking\", \"new_seek\", v)\n            v = re.sub(\"novelty seeking\", \"new_seek\", v)\n            v = re.sub(\"consciousnesnness\", \"conscn\", v)\n            v = re.sub(\"post-use trust\", \"post_trst\", v)\n            v = re.sub(\"post use trust\",\"post_trst\",v)\n            v = re.sub(\"postuse trust\",\"post_trst\",v)\n            v = re.sub(\"emotional experience\",\"emo_exp\",v)\n            v = re.sub(\"agreeableness\",\"agrbns\",v)\n            v = re.sub(\"privacy concerns\",\"priv_cn\",v)\n            v = re.sub(\"privacy concern\",\"priv_cn\",v)\n            v = re.sub(\"cognitive decline\",\"cog_dec\",v)\n            v = re.sub(\"benevolent convenince\",\"ben_conv\",v)\n            v = re.sub(\"enjoyment\",\"enjy\",v)\n            v = re.sub(\"enjoy\", \"enjy\", v)\n            v = re.sub(\"hedonic motivation\",\"hed_motiv\",v)\n            v = re.sub(\"oppenness\", \"open\", v)\n            v = re.sub(\"loyal\", \"loyal\", v)\n            v = re.sub(\"loyalty\",\"loyal\",v)\n            v = re.sub(\"confirmation\",\"confrm\",v)\n            v = re.sub(\"optimism\",\"optim\",v)\n            v = re.sub(\"safety concerns\",\"safe_cn\",v)\n            v = re.sub(\"safety concern\",\"safe_cn\",v)\n            v = re.sub(\"Covid-19 psychological distress\", \"dist_covid\", v)\n            v = re.sub(\"Covid19 psychological distress\", \"dist_covid\", v)\n            v = re.sub(\"Covid 19 psychological distress\",\"dist_covid\",v)\n            v = re.sub(\"psychological distress\",\"dist_covid\",v)\n            v = re.sub(\"green concerns\",\"green_cn\",v)\n            v = re.sub(\"technology anxiety\", \"anxiety\", v)\n            v = re.sub(\"anxiety\",\"anxiety\",v)\n            v = re.sub(\"obedience\", \"obed\", v)\n            v = re.sub(\"empathy\",\"empath\",v)\n            v = re.sub(\"decision comfort\",\"dec_comfrt\",v)\n            v = re.sub(\"confidence\",\"confdnc\",v)\n            v = re.sub(\"decision discomfort\", \"dec_dis_comfrt\", v)\n            v = re.sub(\"comfort\", \"cmfrt\", v)\n            v = re.sub(\"discomfort\",\"discmfrt\",v)\n            v = re.sub(\"insecurity\", \"insec\", v)\n            v = re.sub(\"insecurities\", \"insec\", v)\n            v = re.sub(\"benevolence\",\"benv\",v)\n            v = re.sub(\"technology stress\",\"tech_strss\",v)\n            v = re.sub(\"stress\", \"tech_strss\", v)\n            v = re.sub(\"techno-stress\",\"tech_strss\",v)\n            v = re.sub(\"technostress\",\"tech_strss\",v)\n            v = re.sub(\"techno stress\", \"tech_strss\", v)\n            v = re.sub(\"cognitive resistence\",\"cog_resist\",v)\n                    \n            # demographic \n            v = re.sub(\"age\",\"age\",v)\n            v = re.sub(\"sex\",\"sex\",v)\n            v = re.sub(\"education\",\"edu\",v)\n            v = re.sub(\"income\",\"income\",v)\n            v = re.sub(\"islamic religiosity\",\"religios\",v)\n            v = re.sub(\"culture\",\"cltr\",v)\n                \n            v = \" \".join([word for word in v.split() if word not in stop_words])\n            v = \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in v.split()])\n            #v = \" \".join([stemmer.stem(word) for word in v.split()])\n            dct[k] = v\n    return dct\n\ndef tokenizeToSentences(doc):\n    for k, v in doc.items():\n        \n        if isinstance(v, bytes):\n            v = v.decode('utf-8')\n          \n        v = v.lower()\n        v = v.replace('\\n', ' ')\n        v = re.sub(r'http\\S+www\\S+@\\S+', '', v)\n        #v = \" \".join([str(s) for s in v])\n\n        v = sent_tokenize(v)\n        doc[k] = v\n        \n    return doc\n\nFor Topic modeling, I write a function to generate dictionaries and save them in a .mm file format.\n\ndef generate_dictionary(text, name):\n    \"\"\" \n    As input takes in the text to build the dictionary for and the name of a .mm file\n    \"\"\" \n    \n    dictionary = Dictionary(text)\n    \n    corpus = [dictionary.doc2bow(review) for review in text] \n    \n    filename = f\"{name}.mm\"\n    \n    MmCorpus.serialize(filename, corpus)\n\n    return dictionary, corpus\n\nAdditionally, I want a function that prints the top 50 most frequently appearing words in the corpus:\n\n# ---------------------- START OF CHATGPT CODE\ndef print_top_50_words(corpus, dictionary):\n    total_word_count = defaultdict(int)\n    word_weights = defaultdict(float)\n\n    for word_id, word_count in itertools.chain.from_iterable(corpus):\n        total_word_count[word_id] += word_count\n\n    sorted_tota_words_count = sorted(total_word_count.items(), key = lambda w: w[1], reverse = True)\n\n    tfidf = TfidfModel(corpus)\n\n    for doc in corpus:\n        tfidf_weights = tfidf[doc]  # Calculate TF-IDF for the review\n        for term_id, weight in tfidf_weights:\n            word_weights[term_id] += weight  # Aggregate the weight for the term\n\n    sorted_word_weights = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n\n    # Print the top 50 terms with their weights\n    top_50_words = [(dictionary.get(term_id), weight) for term_id, weight in sorted_word_weights[:50]]\n\n    for word, weight in top_50_words:\n        print(word, weight)\n\n# ---------------------- END OF CHATGPT CODE \n\nI also plan on seeing how python clusters the words (as in, finds similar words) vs me:\n\ndef print_clusters(n_clusters, list_of_words):\n    clusters = {i: [] for i in range(n_clusters)}\n    for word, label in zip(list_of_words, labels):\n        clusters[label].append(word)\n\n    for label, words in clusters.items():\n        print(f\"Cluster {label}:\")\n        for word in words:\n            print(f\"  {word}\")\n        print(\"\\n\")\n\n    # Explain clusters\n    print(\"Cluster explanations based on semantics and ideas:\")\n    for label, words in clusters.items():\n        print(f\"Cluster {label} might be related to:\")\n        for word in words:\n            print(f\"  {word}\")\n        print(\"\\n\")\n\nThis is a function for if you want to use a word embedding (requires some effort, time and machine power!):\n\ndef get_embedding(text):\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    model_bert = BertModel.from_pretrained('bert-base-uncased')\n    \n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=20)\n    with torch.no_grad():\n        outputs = model_bert(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n\nAnd then you use this to get semantically similar words:\n\ndef get_semantically_similar_words(words, threshold=0.7):\n    similar_words = set(words)\n    for word in words:\n        token = nlp(word)\n        for vocab_word in nlp.vocab:\n            if vocab_word.has_vector and vocab_word.is_alpha:\n                similarity = token.similarity(nlp(vocab_word.text))\n                if similarity &gt;= threshold:\n                    similar_words.add(vocab_word.text)\n    return similar_words\n\nSo, how do I find the themes? Essentially, I just tweaked TF-IDF:\n\nclass CustomTfidfVectorizer(TfidfVectorizer):\n    def __init__(self, vocabulary=None, **kwargs):\n        super().__init__(vocabulary=vocabulary, **kwargs)\n        #self.general_keywords = set(general_keywords)\n        \n    def build_analyzer(self):\n        analyzer = super().build_analyzer()\n        return lambda doc: [w for w in analyzer(doc)] #if w not in self.general_keywords]\n    \n    def fit(self, raw_documents, y=None):\n        self.fit_transform(raw_documents, y)\n        return self\n\n    def fit_transform(self, raw_documents, y=None):\n        X = super().fit_transform(raw_documents, y)\n        self.max_frequencies = self._compute_max_frequencies(X, raw_documents)\n        return X\n\n    def transform(self, raw_documents):\n        X = super().transform(raw_documents)\n\n        # Calculate augmented term frequency\n        max_frequencies = self.max_frequencies\n        max_frequencies[max_frequencies == 0] = 1  # Avoid division by zero\n        augmented_tf = 0.5 + 0.5 * (X.toarray() / max_frequencies[:, None])\n        \n        # Penalize general keywords\n        #penalized_idf = self.idf_ * (1 - 0.8 * np.isin(self.get_feature_names_out(), list(self.general_keywords)))\n        \n        # Apply penalized IDF\n        augmented_tfidf = augmented_tf * penalized_idf\n\n        return csr_matrix(augmented_tfidf)\n\n    def _compute_max_frequencies(self, X, raw_documents):\n        max_frequencies = np.zeros(X.shape[0])\n        for i, doc in enumerate(raw_documents):\n            term_freq = {}\n            for term in doc.split():\n                if term in term_freq:\n                    term_freq[term] += 1\n                else:\n                    term_freq[term] = 1\n            max_frequencies[i] = max(term_freq.values())\n        return max_frequencies\n\n\n\n\n\ntry:\n    df = pd.read_csv(\"P2_AR_04.csv\", encoding='utf-8')\nexcept UnicodeDecodeError:\n    try:\n        df = pd.read_csv(\"P2_AR_04.csv\", encoding='latin-1')\n    except Exception as e:\n        error_message = str(e)\n        df = None\n\nClean all the data you‚Äôve gathered the same way the PDF‚Äôs have been cleaned (the preprocess_text() function looks very similar to the cleaning function above!):\n\ndf2 = df.copy()\n\ncolumns_to_preprocess = ['Man_Theme',\n                        'K1','K2','K3','K4','K5','K6','K7','K8','K9','K10',\n                        'F1','F2','F3','F4','F5','F6','F7','F8','F9',\n                        'FNS1','FNS2','FNS3','FNS4',\n                        'METHOD1','METHOD2','METHOD3','METHOD4',\n                        'THEORY1','THEORY2','THEORY3','THEORY4',\n                        'LIMIT1' ,'LIMIT2' ,'LIMIT3', 'Abstract'\n                        ]\n\nfor col in columns_to_preprocess:\n    df2[col] = df2[col].apply(preprocess_text)\n\n\npapers = {}\n\nfor paper_id, filename in name_of_pdfs.items():\n    text = extract_text_from_pdf(filename)\n    papers[paper_id] = text\n\npapers_df = pd.DataFrame.from_dict(papers, orient = 'index', columns = ['paperText'])\npapers_df = papers_df.reset_index(names = ['paperID'])\npapers_df.to_csv('papers_unclean.csv')\npapers_df.head()\n\nThese look like this:",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR"
    ]
  }
]